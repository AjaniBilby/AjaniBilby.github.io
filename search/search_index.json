{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Me","text":"<p>I find it fascinating how the tools we use shape what we create, and then how we can design tools to encourage certain types of behaviour.</p>"},{"location":"#favorite-projects","title":"Favorite Projects","text":"<ul> <li> <p> BNF Parser  2021 </p> <p>Takes in a BNF representation of a syntax and compiles it all the way down to a wasm application for parsing that specific syntax with compiler optimisations applied to it.</p> <p>The compiler also generates type-definitions corresponding to the possible syntax tree layout. To create a much more ergonomic developer experience using the outputted syntax, since the structure will be somewhat known.</p> <p> Website</p> </li> <li> <p> Predictable Bot  2023 </p> <p>A discord bot + website to allow people to post predictions allowing people to place wagers on what they think will happen.</p> <p>The whole system was designed to be robust, ensuring no faux money is lost even in the even of an error. And also ensuring that there is zero down time even during upgrades.</p> <p>Ensuring as the old version is shutdown it completes currently active operations while not interrupting the start up of the new version</p> <p> Website</p> </li> <li> <p> htmX Router  2023 </p> <p>A request router for generating html responses statically similarly to Remix.js's router. But instead being fully server-side rendered, also computing the minimal route returnable to a client given their current route, and sending only the minimal route back to the client and telling them where in the DOM to swap it out.</p> <p> Source</p> </li> <li> <p> Wiki Thesaurus  2023 </p> <p>Uses the simple-wiki dataset to generate a graph structure for wikipedia based on links between articles.</p> <p>From that it then uses Jaccard similarity to take any given input word or entity and attempt to find similar entities. Hopefully producing thesaurus like results (spoiler it's not thesaurus like)</p> <p> Source</p> </li> </ul> <p> All Projects</p>"},{"location":"history/","title":"History","text":""},{"location":"history/#2014","title":"2014","text":"<p>I first started programming when I realised, I could make my own choose your own venture with Windows bat files. This was my first real programming project which I developed completely from scratch, and without any example or tutorials on how to make it.</p> <p>From there I continued playing around with different things, ending up hitting a complexity limit within bat files which forced me to move on. This led me to NodeJS (via realising I hated PHP &amp; WordPress). Since I was still young and still had practically limitless time, I decided I\u2019d take advantage of it. So, I set myself the goal of not using any libraries other than the language\u2019s standard ones. I was going to build it all myself, so I knew how it all worked \u2013 even though I knew it wouldn\u2019t be efficient or nice to work with, I was determined to do it for myself.</p>"},{"location":"history/#2016","title":"2016","text":"<p>I reverse engineered the <code>request</code> library, from there I made my own various small projects experimenting, requiring me to make my own libraries for different things such as: custom-radix conversion for base64 or another encoding; my own flat-file database library; a random library allows for weighted random on selecting items; a compact JSON format encoding numbers and other data in raw binary rather than text.</p>"},{"location":"history/#2017","title":"2017","text":"<p>I wanted more speed, I wanted the ability to run functions in multiple threads, so I created a basic library where you could pass a function and arguments in JSON format to different workers to dispatch tasks. It worked, but it was clunky, horrible, and had a lot of overhead. So, I researched the NodeJS internals and actually did a small presentation in 2017 about it to a local programming community. I realised I\u2019d need to learn C++ and make custom modules to fulfil my goal.</p> <p>So, I started learning C++, and realised it had the speed I wanted \u2013 so I went on similarly to NodeJS and eventually hit a speed cap again. Thus I started investigating how to do multithreading in C++. I originally wanted to make a library for C++, but after all of my research and new knowledge on the inability to manually keep variables synced between threads, and the bulk the API would become, I realised it needed to be its own language.</p>"},{"location":"history/#2018","title":"2018","text":"<p>Thus, we come to 2018. Originally, I started making a language in C++ calling the project <code>fiber</code>, designing it to be compiled to my own byte code which then can be run by a bispoke C++ application. This would then allow me to easily just to certain points during a function for callbacks from other threads. However, I started looking at the performance of bytecode languages like Java and C# verse Go and C++ and realised I would be penalising myself heavily from the start.</p>"},{"location":"history/#2019","title":"2019","text":"<p>I switched tactic, and decided to restart the project calling it <code>Qupa</code> (Queued Parallelism) \u2013 thinking I\u2019d write an interpreter which translates the code into C++ doing that what would turn a horribly complex API much simpler to use due to it being integrated into the language. Then I could manage all of the pointers, and global variables myself to prevent sync problems. But then I rediscovered my old issue. Trying to check/force the synchronisation of variables in C++, there is no ideal way to flush your CPU\u2019s cache to RAM for the other threads to access.</p>"},{"location":"history/#2020","title":"2020","text":"<p>Realising I\u2019m going to have to compile to something lower level than C++ or C, I\u2019ve decided to head down the path of compiling to LLVM IR \u2013 which will then allow me to still execute functions from compiled C/++ which will be helpful for filesystem access, networking capabilities and any other future functionality.</p>"},{"location":"reading-list/","title":"Reading List","text":"<p>This is mostly for myself, I expect no one to go through this</p> <p>This is a list of my favorite articles/books that I've ever read</p> <ul> <li>Steele, G. J. (1977). Debunking the \u201cexpensive procedure call\u201d myth or, procedure call implementations considered harmful or, LAMBDA: The Ultimate GOTO. ACM Annual Conference/Annual Meeting: Proceedings of the 1977 Annual Conference (pp. 153-162). Chicargo: ACM.</li> <li>F. Borretti (). Language Pragmatics Engineering</li> <li>E. W. Dijkstra. (1988). On the cruelty of really teaching computing science Transcript</li> <li>Marcan (2017). Debugging an evil Go runtime bug</li> <li>Fumitake Koga, Ichiro Kishimi (2013). Courage to be Disliked</li> <li>M. Levchin (2023) Shamir Secret Sharing</li> <li>James Iry (2009) A Brief, Incomplete, and Mostly Wrong History of Programming Lnaguages</li> <li>Richard P. Gabriel The Rise of Worse is Better</li> <li>Edward Zitron (2024) The Man Who Killed Google Search</li> </ul>"},{"location":"animation/dots-wasm/memory-layout/","title":"Memory layout","text":"Offset Name Type Details 0 count i32 Number of boids 4 width i32 Screen width 8 height i32 Screen height 12 factorGroup f32 Cohesion factor 16 factorAvoid f32 Avoiding factor 20 factorAlign f32 Aligning factor 24 flock i32 The flocking range 28 avoid i32 The avoid range 32 movSpeed f32 Movement speed 36 turnSpeed f32 Turning speed (d^2) - - - - n*16 + 40 + 0 posX i32 n*16 + 40 + 4 posY i32 n*16 + 40 + 8 velX f32 n*16 + 50 + 12 velY f32"},{"location":"blog/10/","title":"Tabs VS Spaces","text":"<p>When it comes to tabs VS spaces I will always choose tabs with a good IDE. When typing it requires fewer button presses for my prefered visual indentation (2 spaces), as well as allowing for easier indentation changing when re-writing code. Also if you are using a good IDE it will let you change the number of columns that a tab displays at. Allowing for individuals to for with what visually looks like different indentations, while still appearing consistent, since the change in indentation is only a visual effect, rather than a data change.</p>","tags":["Work-flow"]},{"location":"blog/11/","title":"Weighted Random","text":"<p>Weighted randoms are an important feature for any data analysis where the system does not want to flood the user with options, but at the same time, they also do not want to always show the same items. In the use case of advertising, you want to show the user the ad most appropriate for them, however, you also do not want to always show the same ad all the time; you want to keep things fresh. However you also still should show the user the most relevant item most of the time.  </p> <p>This is where weighted random systems come into play; they allow you to show the best answer the majority of the time while still occasionally showing other relevant content. Doing this can actually be good for the system because it can allow the eco-system to discover new trends and remove some feedback loops that might develop otherwise in adaptive recommendation systems. For instance, if the same ad always shows to the same user, and then click on it multiple times it will reinforce the same choice, thus it will display on other peoples recommendations, and the reinforcement will continue. However, it is never showing that original user or any others another recommendation, so eventually you could end up with a system that always displays the same options to all users due to its own feedback loop; and it also has no chance to discover by chance any other correlations. Whereas if a system occasionally shows a recommendation of which does not fit any current trends, but that options now gets clicks under a pattern that previously would have gone undiscovered</p>","tags":["IPT"]},{"location":"blog/11/#foundational-knowledge","title":"Foundational Knowledge","text":"<p>Presume that <code>rand()</code> produces random a number <code>0 &lt; x &lt; 1</code></p> <p>The simplest generation of a random value would be the use of; <pre><code>let y = rand();\n</code></pre> If we instead replace <code>rand()</code> with <code>x</code> you can see how this is a simple <code>1:1</code> relationship. However, if we instead use a more complex geometric relationship like <code>y=x^2</code>, we can see how the average <code>y</code> value between <code>0 &lt; x &lt; 1</code> is less than <code>0.5</code>. Thus when we use that relationship to generate a random number it is more likely to get a number below <code>0.5</code> than above it. <pre><code>let y = rand()**2;\n</code></pre></p>","tags":["IPT"]},{"location":"blog/11/#custom-functions","title":"Custom Functions","text":"<p>If you are randomly picking between an apple and a banana there is a 50-50 chance of either being selected; however, if you want the apple to have double the chance of being selected then you would pick from <code>[apple, apple, bannana]</code>. While this works as a basic implementation it does not create a range of variety and customisation for how to weight each object, what if you want a 13-87 chance between apple and banana? Are you really going to create a list of 100 items then randomly select an item? The whole process can be simplified.</p>","tags":["IPT"]},{"location":"blog/11/#custom-weighted-randoms","title":"Custom Weighted Randoms","text":"<p>Using the previous examples we can say;</p> \\[ \\begin{align*} \\text{Apple} &amp;= 2 &amp;&amp; \\\\ \\text{Bannana} &amp;= 1 &amp;&amp; \\\\ \\text{Total} &amp;= 3 &amp;&amp; \\\\ \\end{align*} \\] <p>So now we can simply stretch our random function to fit over the domain of our options; <pre><code>y = rand() * 3\n</code></pre> Then simply deduce that if <code>y</code> is <code>&lt; 2</code> it must be an apple and otherwise it is a banana.</p>","tags":["IPT"]},{"location":"blog/11/#generalisation","title":"Generalisation","text":"<p>We can now expand on this to make a general algorithm. Assume that the system is given the weights of each option and the options its self. <pre><code>let weights = [...];\nlet opts = [...]\n</code></pre> First, we tally up the total weight <pre><code>let total = 0;\nfor (let weight of weights){\n    total += weight;\n}\n</code></pre> Apply are random function to the domain <pre><code>let y = rand() * total;\n</code></pre> Now deduce which option the result belongs to. <pre><code>for (let index in weights){\n    if (y &lt; weights[index]){\n        return opt[index];\n    }\n\n    y -= weights[index];\n}\n</code></pre></p>","tags":["IPT"]},{"location":"blog/11/#back-to-numeric","title":"Back to Numeric","text":"<p>While it's great to be able to have a weighted random system that picks specific options, if you want actual numeric values then it will be more complex. First, you need to ensure that your <code>weights opts</code> pairs are sorted by <code>opt</code> value. Then we can use a basic interpolation operation. This method will allow you to weight specific numbers within the system and the range of output is defined by the lowest and highest option available. <pre><code>for (let index in weights){\n    if (y &lt; weights[index]){\n        // If it is the last option\n        if (index == opts.length -1){\n            return opts[index];\n        }\n\n        let p = y / weights[index];\n        let val = (opts[index+1] - opts[index]) * p + opts[index];\n\n        return val;\n    }\n\n    y -= weights[index];\n}\n</code></pre></p>","tags":["IPT"]},{"location":"blog/3/","title":"Relational Table DBMS","text":"<p>Recommended: UTF-8 Encoding; Two's Complement Integer</p> <p>This article will not be discussing;</p> <ul> <li>Data Encryption</li> <li>Data replication / distribution</li> <li>Query Languages</li> <li>Sorting</li> <li>Caching</li> <li>Data Conflict resolution</li> <li>Optimizing disk read writes</li> </ul> <p>Since they are all just extra layers of processing on top of what is discussed here, and it can be discussed separately</p> <p>Tables are essentially just a list of tuples (aka records, or rows), of which are stored into a file. However, tables appear to be 2D how does one store it into a linear file. First of all, we need to break down a table into its components.</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#attributes","title":"Attributes","text":"<p>These are the individual fields of a table. They are of set size and length of which makes our job of converting the dataset into a file much easier. Since we know the length of each attribute we can literally just store them one after another, and then read back just the bytes of which belong to each attribute as they are needed.</p> <p>Below is an example of finding the slice points to get a specific attribute within an entire row. <pre><code>let start = 0; // In Bytes\nlet end = 0;   // In Bytes\n\nfor (Attr in Attributes){\n  end += Attr.size;\n\n  if (Attr.name == field){\n    break;\n  }\n\n  start = end;\n}\n</code></pre></p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#data-types","title":"Data Types","text":"<p>Some common table data types and their sizes. * Float: A signed number of which can include non-whole numbers. Made up of 32Bits * Double: A signed number of which can include non-while numbers as well as number impractical to be stored as ints <code>6*(10)^(56)</code>. Made up of 64bits * String: An array of characters * Int: An Integer value encoded using two's complement integer   * Int8: An integer made up of 8 bits   * Int16: An integer made up of 16bits   * Int32: An integer made up of 32bits   * Int64: An integer made up of 64bits * UInt: A positive only integer value   * UInt8: An unsigned integer made up of 8 bits   * UInt16: An unsigned integer made up of 16bits   * UInt32: An unsigned integer made up of 32bits   * UInt64: An unsigned integer made up of 64bits * Boolean: A true/false value of which takes up a whole byte since it is in the smallest division of usable storage space. * Date: The method of storing dates can vary from system to system. You could encode your dates using 32bit ints and UNIX time, or you could use a string and storing it using ISO 8601</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#tuples","title":"Tuples","text":"<p>Since we now know how to read attributes from a tuple, we can use a similar approach to read a tuple from a table. First of all, we need to find the size of each tuple <pre><code>let tupleSize = 0;\nfor (let Attr in Attributes){\n  tupleSize += Attr.size;\n}\n</code></pre></p> <p>Now we can read the tuple in the exact same way</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#primary-and-composite-keys","title":"Primary and Composite Keys","text":"<p>Keys are a way of uniquely identifying individual rows. A primary key is a single attribute of which is unique for every row within the table (i.e. ID). A composite key is when multiple fields in combination are used to uniquely identify a row (i.e. First &amp; Given Name). Both of these key systems can be implemented in multiple ways.</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#depth-key","title":"Depth Key","text":"<p>This is a simple approach where the system reads just the key values of each row until it finds a match with the request, then it returns the whole row. Thus, the time is taken to find a given tuple increase linearly.</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#key-hashing","title":"Key Hashing","text":"<p>The concept of Key Hashing is to convert a given value into a unique number (i.e. James =&gt; 1; Bob =&gt; 2), this means that you can hash a given key to then get the index of the row belonging to that key. Thus, this method can vastly improve the time it takes to get data from the table since it does not need to scan each tuple to see if it's key matches. However, no hashing algorithm is perfect and there will be trade-offs for speed to performance because you will end up with empty indexes between the data of which you have stored.  </p> <p>I.E. if you hash 'Bob' =&gt; 3; and 'Sally' =&gt; 54 that they are the only people in your database you can see that you will now need to store index 1-2 &amp; 4-53 to ensure that the hashing algorithm points to the correct tuple. Since you always use this hashing function to get the index than just read of the index the time taken to read a row is always a constant value, thus being optimal for large databases where size limitations are not as much of an issue.</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#referencing","title":"Referencing","text":"<p>This allows data to be connected almost directly to allow the storing of more complex data. For instance, instead of storing a client's address with every online order they make you can instead just store the client ID as a reference then read the address form the client table.</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#key-referencing","title":"Key Referencing","text":"<p>Uses the systems previously described to find the target row.  </p> <p>If it is a Composite key then you will need to store all key values for the reference to work.</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#exact-referencing","title":"Exact Referencing","text":"<p>Instead of needing to re-find the target row every time, the reference is the exact index of the row. Thus, no extra processing is required to find the target tuple, instead, it can just be read off.</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#compacting-a-table","title":"Compacting a Table","text":"<p>This task removes any empty rows from the database one after another to decrease the size of the table. This operation while space effective will take time, and during processing, no data can reliably be read or written to the table for fear or corruption. Thus, it is a highly expensive processing task. Also, if you are using Key Hashing or Exact Referencing for performance speedups it will break the entire system reading incorrect tuples, and even possibly attempting to read out of the table its self. Thus, it has become a rather out-dated practice only ever really used of archives or personal small-scale databases.</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#foot-note","title":"Foot Note","text":"<p>Since entire tables can be bigger than the RAM of a single computer all modern DBMSs never read the whole table at once. Instead, they read it as a stream of data. For instance, you have an input buffer, you pipe all of your data from your read stream into that, then once you have enough data to make up a tuple, you remove that first tuple amount of data from the buffer and run your operation on it, then you can pipe your result to another file. Thus, you can now work with massive datasets and have massive query results. Now when your operation is done you can just pipe that entire output file to where your result needs to go. Also, it is important to know that this output cache file is not necessary, for instance, you could instead write directly to a table or pipe your results back to whatever requested the event in the first place.</p>","tags":["Data-Structure","IPT"]},{"location":"blog/3/#references","title":"References","text":"<ol> <li>Information Processes and Technology (The HSC Course) - Samuel Davis</li> <li>CS186 Berkeley - Lecture 1</li> </ol>","tags":["Data-Structure","IPT"]},{"location":"blog/4/","title":"Key VS Value Hashing","text":"<p>Both Key and Value hashing aim to convert strings or any other data into a unique number. However, both do it in different ways to be optimal for their purposes.</p>","tags":["IPT"]},{"location":"blog/4/#value-data-hashing","title":"Value / Data hashing","text":"<p>Aims to encrypt data so it is a completely unique number of which is relatively impossible to work backwards (number to string). And there should be no encoding collisions. Since these hashes are normally stored, the length of them is not too much of an issue.</p>","tags":["IPT"]},{"location":"blog/4/#key-hashing","title":"Key Hashing","text":"<p>Allows objects to be stored as an array. Say that you have a DNS object <code>{'google.com': '8.8.8.8', 'facebook.com': '31.13.95.36'}</code>. If you have a ton of domain names, then to find the specific IP of a server requires you to scan all of the keys to find the correct one then return its result. Thus, the larger than your table gets, your request will increase in time exponentially.  </p> <p>Thus, they encode the Key value into a number, then store it as an array. Therefor when you are trying to find <code>google.com</code>'s IP, you just re-encode the string, then you know the exact index of the result. No lookup time required.  </p> <p>A method on encoding that would be to literally convert it to a number.</p> Hex <code>67 6f 6f 67 6c 65 2e 63 6f 6d</code> Decimal <code>5.1679268198614805e+23</code> <p>Of which as you can tell is longer than the longest possible integer, thus we would have a problem indexing it. So, we can try and restricting the character spacing and making our own string to number algorithm. <pre><code>let chars = 'abcdefghijklmnopqrstuvwxyz-_.';\n\nfunction encode(str){\n  let res = 0;\n\n  for (let i=0; i&lt;str.length; i++){\n    let j=chars.indexOf(str[i]);\n    if (j === -1){\n      throw Error('Invalid character:', str[i]);\n    }\n\n    let exp = Math.pow(chars.length, i);\n\n    res += j * exp;\n  }\n\n  return res;\n}\n\nencode('google.com'); //returns 181140446280695\n</code></pre> This is a better value since we can actually index it, and as long as the string isn't just made up of <code>a</code>s it will be fine (it would be like having a number which is just a bunch of zeros). Also, any trailing <code>a</code>s will not be accounted for since this is a little endian encoding, and it would be like having a bunch of zeros in front of your number. This could be fixed by adding <code>1</code> to <code>j</code>. But this isn't the point.  </p> <p>To store <code>google.com</code> we need to have at least <code>181140446280695</code> blank items before this one, or else the index wouldn't actually match. Thus, we are not getting into the realm of intentional hashing collisions.  </p> <p>Within this algorithm (excluding the <code>a</code> problem) there will be no two strings that make the same result, thus you will never have any conflicting pointers (having two sites attempting to use the same index). However, we can case match out an algorithm to better fit our problem.  </p> <p>If we were storing an address book, our key values would only include valid names. Thus, we would not need to be concerned with the fact that <code>aaa</code>, and <code>baaaa</code> may have the same index. As those two keys will never actually occur within the system.  </p> <p>So now we can realize that we only need to care about valid data.</p> <p>This does leave problems for intentionally malicious users to intentionally make conflicts within the system, thus destroying data <pre><code>let chars = 'abcdefghijklmnopqrstuvwxyz';\n\nfunction encode(str){\n  str = str.toLowerCase();\n  let res = chars.indexOf(str[0])+1;\n\n  for (let i=1; i&lt;str.length; i++){\n    res *= chars.indexOf(str[i])+1;\n  }\n\n  return res;\n}\n\n// Test a few names\nlet names = ['Alex', 'Anthony', 'Carla', 'Casandra', 'Jaxon', 'Jackson', 'Jack', 'Jimmy', 'Jim', 'John'];\nfor (let name of names){\n  console.log(name, encode(name));\n}\n</code></pre></p> Name Decimal Alex 1440 Anthony 11760000 Carla 648 Casandra 57456 Jaxon 50400 Jackson 1316700 Jack 330 Jimmy 380250 Jim 1170 John 16800 <p>As you can see we have no conflicts without sample data, and our biggest value is only <code>11760000</code>. Thus, that is the length of our array to store these people efficiently.</p> <p>We can also push our system to the extreme, getting a list of words and testing for conflicts. After executing it (code) we get the results; <pre><code>293555 collided of 370093\n  79.31925218796356% \n</code></pre> Which for our little algorithm whipped up in a couple of minutes isn't too bad, especially since the algorithm is for names and less than 30% of the words in the list names, we can say it was almost successful, but heavily not recommend for real-world situations. We can also conclude that it only made <code>76538</code> unique numbers, of which is actually very bad for our use case since the largest result index was <code>6.868313676231082e+30</code>. However, it is still important to remember that this is still a much better result than our first hashing algorithm of which couldn't even index an eight-character string let alone a 10. Also, our first algorithm given the same problem has only <code>1.176%</code> collision rate, but we cannot actually store any reasonable size strings.</p> <p>All in all, you can see why it is valuable to have specific case-based hashing functions. Possible solutions to some of the problems we have seen are using hashing to predict the location but also storing the key along with the data, thus if a collision occurs the new value can be stored in the next empty index, of which can then be searched for when the predicted location does not match. Thus, you can get the speedups of hashing while still storing a large variety of key values.  </p> <p>Another possible solution which can cause unpredictable results is training a recursive neural network of example keys to generate values that do not conflict with one another but still use the smallest amount of indexes possible. This effect is like the freeloading of a general solution of which may not always work and will increase initial index prediction processing.</p>","tags":["IPT"]},{"location":"blog/4/#foot-note","title":"Foot Note","text":"<p>If you are woundering why there is such a large collision rate with an algorithum that appears to not collide with regular words; it is because of number overflows. If we make a generic algorithm for how many bytes will be needed to store a decimal value based off of string length; $$</p> \\[ 24 ^ {length} \\div 256 \\] <p>So to overflow a 32bit number you only need a length of;</p> \\[ \\begin{align*} {24} ^ {length} \\div 256 &amp;&gt;= 4 &amp;&amp; \\\\ {24} ^ {length} &amp;&gt;= 4 * 256 &amp;&amp; &amp;&amp; \\\\ length * log(24) &amp;&gt;= log(1024) &amp;&amp; \\\\ length &amp;&gt;= \\frac{ log(1024) }{ log(24) } &amp;&amp; \\\\ length &amp;&gt;= 2.18 &amp;&amp; \\\\ \\end{align*} \\]","tags":["IPT"]},{"location":"blog/4/#references","title":"References","text":"<ol> <li>James Curran (NCSS)</li> </ol>","tags":["IPT"]},{"location":"blog/4/#related","title":"Related","text":"<ul> <li>Appendix: Key VS Value Hashing</li> </ul>","tags":["IPT"]},{"location":"blog/5/","title":"5","text":"<p>IPT hide:   - navigation</p>"},{"location":"blog/5/#appendix-key-vs-value-hashing","title":"Appendix: Key VS Value Hashing","text":"<p>Follows on from </p> <p>A way of making better hash functions or adapting an existing one is to limit the range of numbers it can produce. This means that if you had a function that previously had no collisions, but would take up a lot of space you can squish it down to take up less space and hopefully not create any collisions. A way of doing this simply is to use modulo. First of all, we will add a modulo operator to one of our previous encodings. <pre><code>function encode(str, modulo){\n  str = str.toLowerCase();\n  let res = chars.indexOf(str[0])+1;\n\n  for (let i=1; i&lt;str.length; i++){\n    res = (res * chars.indexOf(str[i])+1) % modulo;\n  }\n\n  return res;\n}\n</code></pre></p> <p>Now we need to try and pick a good modulo value that won't create many collisions, and a good way to decide that is to try and find a best possible solution using sample data. Using our previous range as our max value, we will loop through every possible integer modulo and find the number with the least amount of collisions (code).</p>"},{"location":"blog/5/#results","title":"Results","text":"Modulo 023459101120 Conflicts 987654321 <p>We can now see that <code>20</code> is our best value, which is considerably smaller than our previous range of <code>11760000</code>. However, we still have a collision.</p>"},{"location":"blog/5/#hashing-collision-handling","title":"Hashing Collision handling","text":"<p>A way of dealing with hashing collisions is to store key and value pairs instead of just one or the other. Thus if we try and set a new value with the same hash as an existing one, we can recognise that the keys do not match, and then store the data in the next available slot. Thus, also when we try and read by an index as a result of a hash, we can check if the keys match, and if not we can scan each proceeding slot until we find a match. Also if we presume that data is never deleted we can terminate the scan if we find an empty slot, because if the data existed it would exist there.</p>"},{"location":"blog/6/","title":"Object Orientated Database (Static Schema)","text":"<p>Since object-oriented databases are an evolution of Relational table DMBS, it is best to understand RDMBSs before you try to understand OODBs.</p>","tags":["IPT","Data-Structure"]},{"location":"blog/6/#structures","title":"Structures","text":"<p>An OODB schema can be broken down into different sub-sections called structures. Each structure is like a description of a tuple. It describes the name of each attribute as well as it's data type.  </p> <p>For instance, you can describe the structure of a database of which stores GPS locations like so; <pre><code>GPS {\n  latt: float32,\n  long: float32,\n  alt: float32\n}\n\nroot {\n  locations: gps[]\n}\n</code></pre></p> <p>You can see that within locations you can store the individual locations the same as you would a tuple within a table. However, the interesting problem is how you handle having more than one unset length array. How can each of them infinitely expand without colliding with each other?</p>","tags":["IPT","Data-Structure"]},{"location":"blog/6/#n-length-arrays","title":"N-Length Arrays","text":"<p>The simplest way is to chunk the array. In this case, we will pick a chunk size of <code>50</code>. This means that when the array is initiated it will allocate <code>50 \u00d7 GPS.size + 8</code> bytes. This means that the array now has 50 empty slots to fill, once these slots are filled, you can then use the last 6 bytes as a direct pointer to the start of another chunk. Thus, you can continuously expand your array without conflicting with any other arrays.  </p> <p>It can be beneficial to add structure set chunk sizes. For instance if you have an array with a list of all accounts on a website, then you should have your default chunk size quite large to have fewer pointers, However if those accounts then have their own array, i.e. friends, they do not need their chunk size to be so big, since it would be a waste to have massive chunks for only a few items. Personally I prefer the syntax for such arrays to be like this <code>arr[~chunksize]</code></p>","tags":["IPT","Data-Structure"]},{"location":"blog/6/#pointer","title":"Pointer","text":"<p>This allows you to directly reference other data. This is done by storing the exact byte index of where the target starts, as well as what data type is there so that it can be read properly.  </p> <p>It is recommended to state that something is a reference via having a leading <code>&amp;</code>, since that follows on the pre-existing standards created by C</p> <p>This example shows how you can have a list of people, and reference all their friends so that you can read their data easily. <pre><code>person: {\n  given: string[15],\n  family: string[15],\n  street: string[30],\n  suburb: string[10],\n  state: string[4],\n  friends: &amp;person[~5]\n}\n\nroot{\n  people: person[~50]\n}\n</code></pre></p>","tags":["IPT","Data-Structure"]},{"location":"blog/7/","title":"Case Study: Guess Who","text":"<p>Code related to this article can be found here.</p> <p>To implement an online guess who app capable of having millions of popular people listed in it is a large data problem. We also need to be able to add more people to our system as time goes on, so it doesn't become outdated, and we need it to be fast and scalable to allow many people to use our app at the same time.  </p> <p>There are countless solutions to this task, and the one I will be covering uses relational table databases. Table databases are a proven solution to storing large amounts of interlinking data efficiently, and over many years it has been refined to be highly efficient and scalable.</p>","tags":["IPT","Case-Study"]},{"location":"blog/7/#data-structure","title":"Data-Structure","text":"<p>We need to work out how to break now all the linking information that exists in this problem and turn them into restricted field linear data. First we can think of breaking down the problem into rules (questions) and people.</p>","tags":["IPT","Case-Study"]},{"location":"blog/7/#tblquestions","title":"tblQuestions","text":"Field Type Description name String(25) The name of the person","tags":["IPT","Case-Study"]},{"location":"blog/7/#tblpeople","title":"tblPeople","text":"Field Type Description prompt String(45) The actual question <p>It is important to remember that we are only storing facts about people, not what isn't a fact (i.e. \"they don't have brown hair\")</p>","tags":["IPT","Case-Study"]},{"location":"blog/7/#tblmatch","title":"tblMatch","text":"Field Type Description person uint Index reference of the referred person question uint Index reference of the referred person","tags":["IPT","Case-Study"]},{"location":"blog/7/#interfacing","title":"Interfacing","text":"<p>Now that we know how to store our data we need to think about how we can pick a person out of this database as a result, as well as what questions to prompt the user with to find out who their person is.  </p> <p>If we break out into a little thought experiment and presumed that for every 10,000 people one person was in our system. We'll say on average every person has at least 30 rules linked to them. Therefore the size of our matching table would be;</p> \\[ \\frac{ 7 \\times 10 ^ {12} }{10000} \\times 8 \\times 2 \\approx 10GB \\] <p>So we can see that we can't keep a working copy of possible matches for each game, even if with each question the number of possible people halved it would still be impractical to keep any of it in ram. We need to calculate everything on each request then dump all of the working data.</p> <p>So we need to have the client remember what questions they have asked and each type them answer a question they send that result as well as all previous results as part of one request.</p>","tags":["IPT","Case-Study"]},{"location":"blog/7/#matching-people","title":"Matching people","text":"<p>To select the most appropriate question we need to know what people apply to the current rule set, so we can work out which question will best divide the options available. Thus, matching people with the currently know rules isn't just an endgame state, but also a recurring feature.</p> <p>A way of doing this is looping through every single tuple in the matching table then removing any matches that include people that don't follow the rules. To save on read cycles we can do this in one parse of the table. We read the table tuple by tuple finding if the current tuple relates to any of the known rules, as we go we add people to a list as well as what questions apply to them.  </p> <p>If we find any tuple of which links to a question that was marked as wrong by the user, we can remove that person from our cache as then add them to a black-list, so we don't re-add them. <pre><code>let people = {};\nlet blackList = {};\n\nscan: for (tuple of tblMatch){\n  if (tuple.person in blackList){\n    continue scan;\n  }\n\n  if (tuple.question in rules &amp;&amp; rule.value === false){\n    delete people[tuple.person];\n    blackList.push(tuple.person);\n    continue scan;\n  }\n\n  people[tuple.person].push(tuple.question);\n}\n</code></pre></p> <p>However, this does nothing for any rules that are true, however for rules that are true they only need to exist once per person, thus we need to of scanned the entire list to see if there are any links between each person and the rule that rule. We have just created this list <code>people</code>. Now we can run through that and apply the rules of which their value was <code>true</code>. <pre><code>outer: for (person in people){\n  inner: for (rule of rules){\n    if (rule not in person){\n      delete person;\n      continue outer;\n    }\n  }\n}\n</code></pre></p>","tags":["IPT","Case-Study"]},{"location":"blog/7/#finding-appropriate-questions","title":"Finding Appropriate Questions","text":"<p>Now that we have a list of links for people that apply to all the current rules we can choose an appropriate question.</p> <p>The best possible question to ask is one that evenly divides the remaining options because no matter the choice you will remove a considerable amount of possibilities.  </p> <p>To do this we need to loop through our results and count how many times each question occurs. <pre><code>let count = [];\n\nouter: for (let person of people){\n  inner: for (let rule of person){\n    count[rule].num += 1;\n  }\n}\n</code></pre></p> <p>Now we need to find the dividing factor, this will be; $$ \\text{have not} - \\text{have} $$ <pre><code>num = people.length; // Number of people\n\nfor (let opt in count){\n  let no = num - count[rule].num;\n  let yes = count[rule].num;\n\n  opt.val = Math.abs( no - yes )\n}\n</code></pre></p> <p>Now we know what the next best question is because we can just see which value the highest value as ask that.</p> <p>It is important to remember that this system does not have any system to stop it from asking repeated questions.</p>","tags":["IPT","Case-Study"]},{"location":"blog/7/#summary","title":"Summary","text":"<p>It is important to remember that for the first couple of questions the system will still require storing a large percent of the table in ram for processing. If every question halved the number of possible results, then the percent stored in ram would be; $$ 0.5^n $$</p> <p>So after three questions of our previous example the ram used would be $$ 0.5^3 \\times 10GB = 1.25GB $$</p> <p>It also means that by that rule to find one person it would require</p> \\[ \\begin{align*} \\frac{7 \\times 10^9}{10000} \\times 0.5^n &amp;= 1  &amp;&amp; \\\\ 0.5^n &amp;= \\frac{1}{7 \\times 10^9} &amp;&amp; \\\\ 0.5^{-n} &amp;= 7 \\times 10^9 \\\\ -n * log(0.5) &amp;= log(7  * 10^9) &amp;&amp; \\\\ n &amp;= \\frac{ log(7) + log(9) }{log(0.5)} &amp;&amp; \\\\ n &amp;\\approx 30 &amp;&amp; \\\\ \\end{align*} \\] <p>Which would be extremely boring as a player, also it would require a lot of admin to have that many people in the DBMS. Thus, realistically the system would be a lot smaller in scale. However, it is still a good idea to cache the first couple of possible questions as a tree so that they don't need to be recalculated for each user, and instead, they only run operations of which will initially eliminate a lot more results than the first questions would.</p>","tags":["IPT","Case-Study"]},{"location":"blog/9/","title":"Buddy Memory Allocation","text":"<p>A regularly used method for allocating space within a fixed region is Buddy Memory Allocation. This system works like a tree, where each branch either has a boolean value or branches to another set of the same.</p> 8 44 2222 11111111   This means that when you allocated large chunks, or there are multiple chunks allocated next to each other they can merge branches into a single value of true, thus saving memory and compute time since there is no point testing all of the branches if it is already know that they are all true.  ---  ## Defining First of all, we need to set up a branch class since we will have any interlinking entities of this structure and behaviour.  **A** &amp; **B**: Are both branches of the current fork ``BMA``.   **Size**: Is the area of which each sub-branch will cover. <pre><code>class BMA{\n  constructor(parent = null, size = 8){\n    this.parent = parent;\n    this.a = false;\n    this.b = false;\n    this.size = size;\n  }\n\n  get(start, end){}\n  set(start, end){}\n\n  merge(target){}\n\n  find(val, length){}\n}\n</code></pre>  ## Defining First of all, we need to enforce a boolean type to value, as well as calculation the actual size of the allocation being requested. <pre><code>set(start, end){\n  val = (val == true);\n  let size = e - s;\n\n  //...\n</code></pre>  If the value will overwrite an entire branch then, all we need to do is set the branch value to ``val``.   Also if this change makes both branches the same, then there is no point in the divide existing, so we should tell the parent that this path should just be ``val``. <pre><code>  //...\n\n  //Fills sect A\n  if (s == 0 &amp;&amp; e == this.size){\n    this.a = val;\n\n    // There is no reason for this branch to exist anymore\n    if (this.b == val &amp;&amp; this.parent){\n      this.parent.merge(this);\n    }\n\n    //End execution since the desired section only fills this space.\n    return this;\n  }\n\n  //Fills sect B\n  if (s == this.size &amp;&amp; e == this.size + this.size){\n    this.b = val;\n    if (this.a == val &amp;&amp; this.parent){\n      this.parent.merge(this);\n    }\n\n    return this;\n  }\n\n  //...\n</code></pre>  Now we need to handle the complex issues of when a request only partly fills a part of a branch/s. We need to test if the request is touching each branch, and then cascade the set operation down the tree until it completely fills a section/s.   We also need to keep in mind what the branches previous value was because that is what all new subsequent branches value should be unless it is overwritten by the new set operation.   <pre><code>  //...\n\n  // Collides with sect A, and is changing a value\n  if (s &lt; this.size &amp;&amp; this.a != val){\n    if (!(this.a instanceof BMA)){\n      let was = this.a;\n      this.a = new BMA(this, this.size/2); // Create a new sub branch\n      this.a.a = this.a.b = was; // Apply the old value to the default value of the new branches\n    }\n\n    // Confine the range to fit within the sub-branch, and parse the value down the tree.\n    this.a.set(s, Math.min(this.size, e), val);\n  }\n\n  // Collides with sect B, and is chaning a value\n  if (e &gt; this.size &amp;&amp; this.b != val){\n    s -= this.size;\n    e -= this.size;\n\n    if (!(this.b instanceof BMA)){\n      let was = this.b;\n      this.b = new BMA(this, this.size/2);\n      this.b.a = this.b.b = was;\n    }\n\n    this.b.set(Math.max(0, s), e, val);\n  }\n\n  //...\n</code></pre>  Now we just need a final check to run to make sure that if this set caused merging of branches, that it actually continues it's way up the tree. <pre><code>  //...\n\n  if (this.parent &amp;&amp; this.a==this.b){\n    this.parent.merge(this);\n  }\n\n  return;\n}\n</code></pre>  ---  ## Getting There is no point to define anything if you can't read the information back.   Since the request can expose a large section of the allocation information it means that the total value over the range can be a mix of true and false.   Thus instead of returning true and false from this function, we will instead return 1, and -1, respectively, using 0 to mean that the area is a mix of both.  When the request is entirely within only one section. <pre><code>get (s, e){\n\n  // Only in sect A\n  if (s &lt; this.size &amp;&amp; e &lt;= this.size){\n    // If a is another branch, then parse forward the request\n    if (this.a instanceof BMA){\n      return this.a.get(s, e);\n    }\n\n    // If a is true return 1, else return -1\n    return this.a ? 1 : -1;\n  }\n\n  // Only in sect B\n  if (s &gt; this.size){\n    if (this.b instanceof BMA){\n      return this.b.get(\n        s-this.size, //Make the pointers relative\n        e-this.size\n      );\n    }\n\n    return this.b ? 1 : -1;\n  }\n\n//...\n</code></pre>  Now we need to write a case for if the request covers at least some of both branches.   This means we need to test the composition of which covers each branch, and if the two values don't match then return 0, otherwise we can just return their shared value.   &gt; Remember, if one branch is a mix of both values there is no point in testing the value of the other since any possible combination will also just result in a mix of the two. <pre><code>//...\n\n  let a = null;\n  let b = null;\n\n  // If the request collides with branch A\n  if (s &lt; this.size){\n    if (this.a instanceof BMA){\n      a = this.a.get(\n        s,\n        Math.min(this.size, e) // Ensure that the parsed end pointer isn't going out of the branch\n      );\n    }else{\n      a = this.a ? 1 : -1;\n    }\n\n    if (a === 0){\n      return 0;\n    }\n  }\n\n  // If the request collides with branch B\n  if (e &gt; this.size){\n    if (this.b instanceof BMA){\n      b = this.b.get(s-this.size, e-this.size);\n    }else{\n      b = this.b ? 1 : -1;\n    }\n\n    if (b === 0){\n      return 0;\n    }\n  }\n\n\n\n  // If one of the branches's didn't actually collide with the request\n  if (a === null){\n    return b;\n  }\n  if (b === null){\n    return a;\n  }\n\n  if (a === b){\n    return a;\n  }else{\n    return 0;\n  }\n\n}\n</code></pre>  ---  ## Finding space There is no point to an allocation system if you can't find a space to put some data. So our request will just state the desired size.   We will also make this function more versatile by allowing the request to specify the value it is trying to find, and thus how much space that value takes up.    We also should always make sure that we return the tightest fitting space available because then it allows the bigger available spaces to be filled by future requests.  <pre><code>hit (val, size){\n  // The request won't fit, at this depth, so there's no point\n  // Checking deeper branches since they will just have less\n  // space\n  if (this.size &lt; size){\n    return NaN;\n  }\n\n  // If the request matches an available branch\n  if (this.a === val){\n    return {pos: 0, size: this.size}\n  }else if (this.b === val){\n    return {pos: this.size, size: this.size}\n  }\n\n\n  let a = NaN;\n  let b = NaN;\n  if (this.a instanceof BMA){\n    a = this.a.hit(val, size);\n\n    // Tight fit found\n    // There will be no better solutions, only equal best\n    if (a != NaN &amp;&amp; a.size == size){\n      return a;\n    }\n  }\n  if (this.b instanceof BMA){\n    b = this.b.hit(val, size);\n\n    if (b != NaN &amp;&amp; a.size == size){\n      return b;\n    }\n  }\n\n\n  // If an option is NaN, then return the other option\n  let na = isFinite(a) &amp;&amp; isNaN(a);\n  let nb = isFinite(b) &amp;&amp; isNaN(b);\n  if (na &amp;&amp; nb){\n    return NaN;\n  }\n  if (!na &amp;&amp; nb){\n    return a;\n  }\n  if (na &amp;&amp; !nb){\n    return b;\n  }\n\n  // Both options a &amp; b will work,\n  // Select which ever one is smallest,\n  // Prioritising a\n  if (a.size &lt; b.size){\n    return a;\n  }\n  if (b.size &gt; a.size){\n    b.pos += this.size; // Make the pos relative to this depth\n    return b;\n  }\n  if (a.size === b.size){\n    return a;\n  }\n\n  throw new Error()\n}\n</code></pre>  However, this allocation system doesn't automatically example it's self when necessary to write new data. This will add a new parent class to allow this.  <pre><code>class BMAX{\n  constructor(length){\n    this.root - new BMA();\n    this.length = this.root.size *2;\n  }\n\n  set (s, e, val){\n    return this.root.set(s,e,val);\n  }\n  get (s,e){\n    return this.root.get(s,e);\n  }\n  find(size, val){\n    return this.root.hit(val === true, size);\n  }\n\n  extend(amount = 1){\n    let ol = this.length;\n    if (amount &lt; 1){\n      amount = 1;\n    }\n\n    // Make the current root into a branch of a new root\n    while(this.length - ol &lt; amount){\n      let t = new BMA(undefined, this.length);\n      t.a = this.root;\n      t.a.parent = t;\n      this.root = t;\n\n      this.length = this.root.size*2;\n    }\n  }\n}\n</code></pre>  Now if the recursive search function doesn't return any results, we can tell the allocation table to expand, as well as expanding out initial storage location to now have space to put the new data.  ---  [code](https://github.com/Hobgoblin101/Hobgoblin101.github.io/tree/master/code/9)","tags":["Data-Structure","IPT"]},{"location":"blog/apex-ssbm-nov22/","title":"A Case Study on Apex SBMM","text":"<p>Edits</p> <p>31/10: Updated the discussion section due to some math errors, no other sections are affected</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#introduction","title":"Introduction","text":"<p>I've spent the past 300days (actually 296) recording my daily stats on Apex to see if I could find any trends hoping to understand more about their skill based match making (SBMM). I was inspired by istiri7's post about recording 500 games of solo'q to learn more about SBMM, and wondered if there are any trends you could see over the seasons, rather than at the macro level of 500 individual games.</p> <p>In this article I'm going to go in-depth over what I've found so far; the reason I'm posting this before I reach 300, or even before I reach my original goal of a year - is because there is a lot of talk about SBMM at the moment not only about Apex, but other games such as Overwatch and Modern Warfare as well. And I wanted to share what I had found so far, before I miss my chance to be part of this discussion.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#data-collection","title":"Data Collection","text":"<p>All of the input data is simply collected in a single Excel sheet. This includes the date, mode, total kills, and total deaths for that season so far. Every time I switch mode, I log this data (i.e. if I go from play trios to ranked) - and at the end of each day I log the data. This allows me to derive the number of kills and deaths per mode per day. Which is seen on the right hand side. This information can then be used to find the lifetime data per mode as well as other information discussed in this article.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#lifetime-statistics","title":"Lifetime Statistics","text":"<p>All statistics from this section on are only from solo-q trio games, the kills and deaths from other modes such as ranked, duos, and times I played with friends have been omitted from these results.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#lifetime-kdr","title":"Lifetime KDR","text":"<p>The light blue dots represent a single day, with the blue like connecting them showing consecutive days of play. The yellow line is the KDR based on the sum of kills and deaths from the last 7 days (rather than an average of the daily KDR of the last 7 days). Similarly the green line shows the total KDR of the past 30days. The salmon line shows the top 25th percentile daily KDR from the past 30 days (explained more in the next section).</p> <p>There aren't any inherently obvious trends visible from this chart, other than there appears to be a sine-wave like trend occurring, so there likely is a seasonality measurement for this data.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#kdr-percentiles","title":"KDR Percentiles","text":"Per-Season Daily KDRNormalised <p>These graphs use a sliding window of the past 30days of KDR values. Based on these samples it takes the value at a certain percentile for each window. So the 75% line is the KDR value marking the top 25% of your KDRs over the last 30 days.</p> <p>The normalised view makes the 25% and 75% values relative to the median (50%) KDR.</p> <p>There is not much information that can be derived from this, other that something appears to have changed on the 9th of September in relation to the match making. Nothing in my daily life would give reason to a sudden gradual growth in my skill.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#seasonality","title":"Seasonality","text":"Factor Seasonality (days) Daily KDR 71 Weekly KDR 21 30 Day Median KDR 74 30 Day Top 25% KDR 60 <p>When using the <code>Daily KDR</code>, <code>30 Day Median KDR</code>, <code>30 Day Top 25% KDR</code> to estimate the seasonality of the data<sup>1</sup> - they all agree on a \\(\\approx 73 \\text{ day}\\) interval between seasons. This means your peak performance in terms of KDR will occur about every \\(73\\text{ days}\\). This is very interesting as the average Apex season is about \\(90\\text{ days}\\). However my university semesters are 11-12 weeks each, if we say I'm only in uni for 11 weeks because I don't focus during O-week, then we get \\(11 \\times 7 = 77\\text{ days}\\) which is almost our \\(73\\text{ days}\\) we are looking for. However my semesters are not evenly spaces apart, so if my university semesters were the cause of the seasonality that could interfere with the estimate calculated.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#individual-season-statistics","title":"Individual Season Statistics","text":"","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#statistics-per-mode","title":"Statistics per Mode","text":"<p>The below statistics are pull straight from the Apex menu's for statistics per season. <code>All Modes</code> is your overall stats for the season, <code>Ranked</code> is the stats for each ranked season, <code>Non-Ranked</code> is calculated based on removing your ranked results from your <code>All Modes</code> results. Hence <code>Non-Ranked</code> will include <code>trios</code>, <code>duos</code>, <code>solo-q</code>, and partying statistics.</p> Average DamageKDRWin Rate%Top 5 Placements%Games Played <p>All Modes  Ranked  Non-Ranked </p> <p>All Modes  Ranked  Non-Ranked </p> <p>All Modes  Ranked  Non-Ranked </p> <p>All Modes  Ranked  Non-Ranked </p> <p>All Modes  Ranked  Non-Ranked </p> <p>Looking at the <code>KDR</code> and <code>Average Damage</code> per season, they both tell a similar story. Around season 11 SBMM kicked into high gear, as soon as my KDR reached 2, clearly the system intentionally started putting me into matches aimed to make the game hard enough for me to bring me down to the level of those around me. You can also compare this to the ranked scores and see that I was still improving, which can also be backed up with my Kovaaks scores at the time as well. Clearly I did not hit my skill ceiling and instead something else is happening. I suspect around the 2KDR mark is when Big-Brother matchmaking<sup>2</sup> really starts to kick in. Before 2KDR the match maker likely just puts you into lobbies with increasing skill as you progress, but once you've hit the 2KDR point it decides it can no longer find matches for you at the frequency desired, so instead it puts you into a lobby at a lower level than you, but with teammates even lower level that the lobby's in hopes of weighing you down to make your team's level equivalent to the lobby.</p> <p>Interestingly though <code>win rate</code> does not appear to be affected to a similar degree to <code>KDR</code> and <code>Average Damage</code>, instead what is interesting is to look at the percent number of times placed in the top 5. This metric is held incredibly constant, and even more suspicious is that it tends around the 25% mark, which also happens to be the statistical chance of coming 5th out of 20 if all things are equal. This shows that while the Apex SBMM system may not be good at making individual matches balanced, as a whole it appears to be remarkably good if we assume it's goal is to optimise fair top 5 placements.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#per-season-kdr-over-time","title":"Per Season KDR over time","text":"<p>All statistics from this section on are only from solo-q trio games, the kills and deaths from other modes such as ranked, duos, and times I played with friends have been ommited from these results.</p> Per-Season Daily KDRZoomed <p></p> <p>Skips the first 10days</p> <p></p> <p>Very peculiarly days 1-10 of each season always have much higher KDR than the rest of the season. If a few, or even the majority (but not all) had a higher KDR you could easily claim it's a few lucky results, as the earlier in the season, the less games have been played, so the higher variability present in results. However the fact all of the seasons start around the 4KDR range, then all follow a similar path back down to normalising to ~2KDR it raises an interesting question.</p> <p>If you couple these results with the results from the Statistics per Mode, and interesting thought comes to mind. The match making clearly attempts to prevent you from going above 2KDR, so why does it always fail to do so in the first week of each season? I believe the answer is what data the match maker is running off. I believe the match maker's internal values reset every season - so the first day of each season you play, you could almost think of like if Apex didn't have match making.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#week-day-statistics","title":"Week Day Statistics","text":"<p>Overall there is around 0.5 KDR difference based on what day of the week you play. It seems tuesday and friday appear to be the best days to play but only by a slim margin. From this it is quite clear, what day of the week you play doesn't appear to impact your match making a significant amount, most likely time of day would have a large impact, however that was not tracked.</p> <p></p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#conclusion","title":"Conclusion","text":"<p>SBMM clearly tries to prevent any players from dominating the lobbies they are in, however the SBMM system appears to create a skill ceiling instead of allowing skill growth to be met with a logarithmic return. This will likely destroy any sense of player skill progression within this this mode, as they are not able to experience any improvements in their performance once they reach a certain point. SBMM has a clear focus on top 5 placements at it's core, with KDR as a secondary core metric, with damage likely not playing a part and instead being a corelation with kills.</p> <p>There does not appear to be a periodic change in performance over time if we assume my university semesters are the main driving factor. My university has it's semesters largely out of sync with other universities so I doubt other players would experience the same cycle as the majority of the player base isn't going through the same labour cycle. Let alone I doubt the majority of players are uni students anyway.</p> <p>There have been signs of the developers changing the match making over time (more discussed in Further More), it is clearly an on going process of refinement, and the developers aren't sitting on their lorals.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#discussion","title":"Discussion","text":"<p>Overall I believe SBMM should remain a part of Apex, however I do not agree with the current state of SBMM. To explain this let's look at two potential options.</p> <p>A World Without SBMM</p> <p>In this world players would simply sit in one massive queue, as soon as there are 60 people, the first 60 people would get into the same game, and the rest would wait for the next lobby. First in first serve, no skill consideration. This means everyone from a player learning to use a mouse, to iiTzTimmy, to MattyOW would be in the same lobby. I think people under-estimate how unenjoyable this would be after a week. This means you could very likely end up always playing against people who don't even know how to move or shoot - if you have a good grasp on the mechanics of the game you're going to be shooting targets which basically don't fight back all day and it will get very boring.</p> <p>If you don't believe me, and you're in the top 10-20% of players, I implore you to boot up a old shooter without SBMM. I recently went back to Xonotic (a quake like shooter), and for the first couple of hours it was fun as heck. Getting 3-10x the score of my opponents every game. But eventually it got kind of boring because I was never really challenged.</p> <p>Also for those of you who aren't high skill level, thing about the fact for every 2KDR player, there is either one player with 0.5, or two players with 1KDR. That's just how the maths works out. If we propose a situation with 60 players that play 100 matches - and to make the math simple being the last alive counts as a death, and there are no respawns.</p> \\[ \\begin{gathered} \\text{Propose all top 10% player have a 8KDR} &amp;&amp; \\\\ \\begin{aligned} k + d &amp;= 100 &amp;&amp; \\\\ \\frac{k}{d} &amp;= 8 &amp;&amp; \\\\ \\end{aligned} \\end{gathered} \\] \\[ \\begin{gathered} \\therefore \\text{A top 10% player's individual stats would be} \\\\ \\begin{aligned} \\frac{k}{100 - k} &amp;= 8 &amp;&amp; \\\\ k &amp;= 8 \\times (100 - k) &amp;&amp; \\\\ k &amp;= 800 - 8k &amp;&amp; \\\\ 9k &amp;= 800 &amp;&amp; \\\\ k &amp;= 88.\\overline{888} &amp;&amp; \\\\ \\end{aligned} \\end{gathered} \\] \\[ \\begin{gathered} \\text{As a group the top 10% players would then make up:} \\\\ \\begin{aligned} k + d &amp;= 100 &amp;&amp; \\\\ d &amp;= 100 - k &amp;&amp; \\\\ d &amp;= 11.\\overline{111} &amp;&amp; \\\\ \\end{aligned} \\end{gathered} \\] \\[ \\begin{align*} k &amp;= 88.\\overline{888} \\times 60 \\times 10\\% &amp;&amp; \\\\ &amp;= 533.\\overline{333} &amp;&amp; \\\\ :&amp;= 533 &amp;&amp; \\\\ d &amp;= 11.\\overline{111} \\times 60 \\times 10\\% &amp;&amp; \\\\ &amp;= 66.\\overline{666} &amp;&amp; \\\\ :&amp;= 66 &amp;&amp; \\\\ \\end{align*} \\] \\[ \\begin{align*} k &amp;= \\frac{ 60 \\times 100 - 533 }{ 60 \\times 90\\% } &amp;&amp; \\\\ :&amp;= 101 &amp;&amp; \\\\ d &amp;= \\frac{ 60 \\times 100 - 66 }{ 60 \\times 90\\% } &amp;&amp; \\\\ :&amp;= 109 \\end{align*} \\] \\[ \\begin{gathered} \\text{A top 10% player would have the stats} \\\\ \\begin{aligned} k &amp;= 533 &amp;&amp; \\\\ d &amp;= 66 &amp;&amp; \\\\ \\text{KDR} &amp;= \\frac{533}{66} &amp;&amp; \\\\ &amp;= 8.0\\overline{75} &amp;&amp; \\\\ \\end{aligned} \\end{gathered} \\] \\[ \\begin{gathered} \\text{A bottom 90% player would have the stats} \\\\ \\begin{aligned} k &amp;= 101 &amp;&amp; \\\\ d &amp;= 109 &amp;&amp; \\\\ \\text{KDR} &amp;= \\frac{101}{109} &amp;&amp; \\\\ &amp;:= 0.927 \\end{aligned} \\end{gathered} \\] <p>Honestly that's not such a bad break down, however this assumes there are only 8KDR players, and the rest with no middle ground. In the real world there would be a distribution of skill levels, meaning the lower down player will end up with a much lower KDR.</p> <p>A World with \"Perfect\" sbmm</p> <p>In this world every match is perfectly balanced, all teams are perfectly equal, crafted to ensure all teams share the same weaknesses and strengths. In this world you will never feel a sense increase in skill. Ask you get better, you will be match made so that it won't make a difference in the out-come of your games. Because all teams are equal.</p> <p>The only sense of progression you'll have is your moment to moment performance, how quickly you beam someone, how accurate are you shots, how sick of a wall bounce into superglide tap strafe. However none of that will actually impact how well your games perform over all, because who ever your versing will be able to counter that because all teams are balanced.</p> <p>Neither of these worlds results in a long living game. The best option is something inbetween that allows players to have fair games, while still allowing for a sense of progression. That's specifically why earlier I complained about a skill ceiling, and instead requested a logarithmic skill progression. This allows the majority of players to experience a sense of progression, however as you get closer and closer to the higher elo, your sense of progression will slow down. Because without we end up with the 8:0.9 KDR situation.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#ranked-vs-sbmm","title":"Ranked vs SBMM","text":"<p>Technical Director on Apex at Respawn Samy Duc</p> <p>[.. sbmm] is just matchmaking with skill rating, so by definition just matchmaking [...]</p> <p>The progression on top is to give you a sense of achievement nothing more, you are what you are and progressing your actual skill at the game take way longer than the short loop of reward/dopamine you get when finishing a single game.</p> <p>To get back at the question, pubs usually does not get a progression system and therefore is less sticky to players compared to ranked. But ranked can be scary, because people care about their progression number so you have to offer a way for people to engage with the game [...]</p> <p>To avoid the 8:0.9 SBMM problem, we need to include skill ratings to normal match making. But similarly ranked has become more like normal play, because they've made skill based system with an emphasis on fun. So you have a fun based mode with an emphasis on skill, and a skill based mode with an emphasis on fun, so the only difference between ranked and norms is the exact parameters and whether or not the skill rating is shown (i.e. rank). Since seems kind of like a sad rebranding of two things hoping people enjoy both namings.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#improvements","title":"Improvements","text":"<p>Personally I think SBMM does need to be in normal game play, but big brother match making should be removed, or heavily nerfed. Yes that will increase the likely hood of the 8:0.9 problem, however we're only increasing it in the top end which creates a interesting side affect.</p> <p>For those who are in the middle of the pack, who would be versing people who are normally big-brothered into their lobbies. When they verse those players they will perform worse, that will bring down their rating, they will verse the lower players, have some fun easier frags and get their stats up to head butt with the high elo again. The high elo remain able to have a sense of progression, as their KDR can still feasibly go up, and they don't have the sensation of increasing sand bags at their heels pulling down their soul.</p> <p>I think there is a massive reason why all of the solo-q players play a lot of no-fill, it's because the big brother affect is so bad, they may as well have no one, rather than low elo players to carry.</p>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/apex-ssbm-nov22/#further-more","title":"Further More","text":"<p>There are a few double peaks present in histogram KDR charts which hints at some interesting findings. First of all, an imperfect match maker, which not all games a balanced, but on average the results balance out - these results will follow a normal curve according to statistics fundamentals. However the below charts have double peaks which can only mean one thing. The system changed / there are multiple statistical systems occurring at once. If the histogram showed a improvement or degradation of skill, the graphs should be skewed, because improvement is a gradual process, not a jump in skill. However a double peak implies drastic changes, like the match maker being tweaked.</p> <p>However most of these double peaks happen around the season end/start mark, which is when KDRs spike as seen in the Statistics per Mode, this means either the match maker hasn't been updated drastically, and instead all changes are fine tuning. Or all major changes only happen on the start of a season. Either than or the match making stats just reset every season and match making hasn't changed since S11 - which I doubt.</p> MaySeptember <p></p> <p></p> <ol> <li> <p>The seasonality is calculated using Excel's build-in functions.\u00a0\u21a9</p> </li> <li> <p>Big-Brother Matchmaking is the term to describe a match making algorithm intentionally pairing a high skill player with lower skill players in an attempt to create a team of equivalent skill of the other(s). This then creates a team dynamic where the higher skill player has to play the role of the big brother in the team.\u00a0\u21a9</p> </li> </ol>","tags":["Apex Legends","Match Making","SBMM"]},{"location":"blog/async-js-performance-apr23/","title":"Async functions are needlessly killing your Javascript performance","text":"<p>While numerous articles offer quick tips to enhance your async JavaScript performance using extra Promise API features, this discussion focuses on how program structure tweaks can lead to significant speed improvements. By optimizing your code, you could potentially achieve 1.9x or even 14x speed boosts.</p> <p>I believe the untapped performance potential in asynchronous JavaScript features is due to the V8 engine not providing the expected level of optimization for these features; and there are a few key indicators that suggest this possibility.</p>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#context","title":"Context","text":"<p>You can skip this section if you want, but here's a brief overview of the context. I've been working with a bnf-parser library that currently needs a complete file to be loaded for parsing it into a BNF-specified syntax tree. However, the library could be refactored to use cloneable state generators, which output file characters sequentially and allow for copying at a specific point to resume reading later.</p> <p>So I tried to implementing it in Javascript be able to parse large +1GB files into partial syntax trees for processing large XML, just partly for fun, partly because I know also soon I'll need to be implementing something similar in a lower level language and this could be good practice.</p>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#the-case-study","title":"The Case Study","text":"<p>I aimed to create a layer between the readable data stream from disk and allowing iteratively calling forward for small text portions with limited backtracking. I implemented a Cursor that iterates forward, returning the passed-over characters as a string. Cursors can be cloned, and clones can independently move forward. Importantly cursors may need to wait for data currently streamed to become available before returning the next substring. To minimize memory usage, we discard unreachable data - implementing all of this into a async/await pattern to avoid complex callback chains or unnecessary event loop blocking.</p> <p>Side note: We use pooling for caching, placing each chunk read from the disk into an array and manipulating the array to free cached data. This method reduces resize operations and string manipulation. However, it can cause NodeJS to report false memory usage, as chunks allocated by the OS are sometimes not counted until manipulated within the application domain.</p> <p>The cursor features an async read call, asynchronously connecting to a StreamCache to read from the cache. Multiple cursors may attempt to read the latest unavailable information, requiring a condition variable lock - an async call to a PromiseQueue is used to manages this.</p> <p>Reading a <code>1GB</code> file in <code>100-byte</code> chunks leads to at least <code>10,000,000 IOs</code> through three async call layers. The problem becomes catastrophic since these functions are essentially language-level abstractions of callbacks, lacking optimizations that come with their async nature. However, we can manually implement optimizations to alleviate this issue.</p>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#testing","title":"Testing","text":"<p>So let's go through the base implementation, then a few different variations and optimisations; or you can skip ahead to the #results then work your way backwards if you prefer.</p> <p>A quick note about the testing methodology: Each test ran 10 times consecutively, starting from a cold state. The first result was consistently slower, while the other nine were nearly identical. This suggests either NodeJS temporarily saves optimized code between runs, or the NAS intelligently caches the file for quicker access. The latter is more likely, as longer durations between cold starts result in slower initial executions.</p> <p>The test file used is here (streamed as a standalone XML file).</p>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#full-async","title":"Full Async","text":"<p>So we have a cursor which we can call next on, which forwards the request to the StreamCache - which then handles all of the actual read behaviour.</p> <pre><code>class Cursor {\n  // ...\n  async next(highWaterMark = 1): Promise&lt;string&gt; {\n    return await this._owner._read(this, highWaterMark);\n  }\n  // ...\n};\n</code></pre> <p>We then have our main file which just creates a StreamCache, adds a cursor, and piping a <code>fs.createReadStream</code> in a kind of backwards way to the normal piping API, but this is due to the way <code>StreamCache</code> has been implemented to allow for NodeJS and WebJS readable stream API differences.</p> <p>The cursor is added before piping to ensure the first bytes of data can't be read into the cache, then dropped because of it being inaccessible by any cursors</p> <pre><code>let stream = new experimental.StreamCache();\nlet cursorA = stream.cursor();\nstream.pipe_node(fstream);\n\nasync function main() {\n  console.time(\"duration\");\n\n  while (!cursorA.isDone()) {\n    let val = await cursorA.next(100);\n    read += val.length;\n  }\n\n  cursorA.drop();\n\n}\nfstream.on('end', ()=&gt;{\n  console.timeEnd(\"duration\");\n});\n\nmain();\n</code></pre>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#wrapper-optimisation","title":"Wrapper Optimisation","text":"<p>In the cursor before we could see we had an async function basically just acting as a wrapper, if you understand the async abstraction you'd know an async function just returns a promise, so there is no actual need in creating this extra async function, and instead we can just return the one created from the child call. (This has a level of performance benefit it really shouldn't :D)</p> <p>To: <pre><code>class Cursor {\n  next(highWaterMark = 1): Promise&lt;string&gt; {\n    return this._owner._read(this, highWaterMark);\n  }\n};\n</code></pre></p>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#inlined","title":"Inlined","text":"<p>In this case we pretended to be a compiler, and inlined our own function, so we literally just embedded the functionality of <code>StreamCache._read</code> into where it was being called, which completely broken our public private attribute protections  If only there was a compiler like Typescript to do inlining safely for us </p> <pre><code>let stream = new experimental.StreamCache();\nlet cursorA = stream.cursor();\nstream.pipe_node(fstream);\n\nasync function main() {\n  console.time(\"duration\");\n\n  while (!cursorA.isDone()) {\n    if (cursorA._offset &lt; 0) {\n      throw new Error(\"Cursor behind buffer position\");\n    }\n\n    while (cursorA._offset &gt; stream._total_cache - 100) {\n      if (stream._ended) {\n        break;\n      }\n\n      await stream._signal.wait();\n    }\n\n    let loc = stream._offset_to_cacheLoc(cursorA._offset);\n    if (loc[0] &gt;= stream._cache.length) {\n      return \"\";\n    }\n\n    let out = stream._cache[loc[0]].slice(loc[1], loc[1]+100);\n    cursorA._offset += out.length;\n    read += out.length;\n  }\n\n  cursorA.drop();\n\n}\nmain();\n\nfstream.on('end', ()=&gt;{\n  console.timeEnd(\"duration\");\n});\n</code></pre>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#async-with-peaking","title":"Async With Peaking","text":"<p>If all else fails, avoid async when possible. So in this case I added a few functions. Peak will tell me if I can read without waiting, and in which case <code>_skin_read</code> is safe to call. Otherwise go back to calling the async method.</p> <pre><code>let stream = new experimental.StreamCache();\nlet cursorA = stream.cursor();\nstream.pipe_node(fstream);\n\nasync function main() {\n  console.time(\"duration\");\n\n  while (!cursorA.isDone()) {\n    let val = cursorA._skip_read(100);\n    if (cursorA.isDone()) {\n      break;\n    }\n    read += val.length;\n    peaked += val.length;\n\n    if (val == \"\") {\n      let val = await cursorA.next(100);\n      read += val.length;\n    }\n  }\n\n  cursorA.drop();\n}\nmain();\n\nfstream.on('end', ()=&gt;{\n  console.timeEnd(\"duration\");\n});\n</code></pre> <p>In this use case this actually save a lot of time because a large amount of the calls didn't actually need to wait due to the load chunk sizes being so large.</p> Bits Read Via Async 919417 Via Peaking 1173681200 Total 1174600617","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#disk-read","title":"Disk Read","text":"<p>As with all good tests, we need a base line - so in this case we don't even have an active cursor, we literally just let data flow in and out of the <code>StreamCache</code> as fast as possible giving us the limitation of our disk read, plus the <code>alloc</code> and <code>free</code> overhead as we add and remove cache pools.</p> <pre><code>let stream = new experimental.StreamCache();\nlet cursorA = stream.cursor();\nstream.pipe_node(fstream);\n\nasync function main() {\n  console.time(\"duration\");\n  cursorA.drop();\n\n}\nmain();\n\nfstream.on('end', ()=&gt;{\n  console.timeEnd(\"duration\");\n});\n</code></pre>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#callback","title":"Callback","text":"<p>Finally we need a test to make sure this isn't a de-optimisation bug, if we go back to the callback hell days, however do we fair?  </p> <p>Note: I didn't rewrite the <code>signal.wait()</code> as trying to create an optimised call back system inside a for loop will be hell on earth to implement. And yes we do need a while loop, because it might take more than one chunk to load in to fulfill the requested read - chunk sizes can be weird sometimes and inconsistent, plus maybe you just want a large chunk read at once </p> <pre><code>export class StreamCache {\n  async read(cursor: Cursor, size = 1, callback: (str: string) =&gt; void): Promise&lt;void&gt; {\n    if (cursor._offset &lt; 0) {\n      throw new Error(\"Cursor behind buffer position\");\n    }\n\n    // Wait for more data to load if necessary\n    while (cursor._offset &gt; this._total_cache - size) {\n      // The required data will never be loaded\n      if (this._ended) {\n        break;\n      }\n\n      // Wait for more data\n      //   Warn: state might change here (including cursor)\n      await this._signal.wait();\n    }\n\n    // Return the data\n    let loc = this._offset_to_cacheLoc(cursor._offset);\n    if (loc[0] &gt;= this._cache.length) {\n      callback(\"\");\n    }\n\n    let out = this._cache[loc[0]].slice(loc[1], loc[1]+size);\n    cursor._offset += out.length;\n    callback(out);\n  }\n}\n</code></pre> <pre><code>function ittr(str: string) {\n  read += str.length;\n  if (cursorA.isDone()) {\n    cursorA.drop();\n    return;\n  }\n\n  stream.read(cursorA, 100, ittr);\n}\n\nasync function main() {\n  console.time(\"duration\");\n  ittr(\"\");\n}\nmain();\n\nfstream.on('end', ()=&gt;{\n  console.timeEnd(\"duration\");\n});\n</code></pre>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#results","title":"Results","text":"Case Duration (Min) Median Mean Max #Full Async 27.742s 28.339s 28.946s 35.203s #Async Wrapper Opt 14.758s 14.977s 15.761s 22.847s #Callback 13.753s 13.902s 14.683s 21.909s #Inlined Async 2.025s 2.048s 3.037s 11.847s #Async w/ Peaking 1.970s 2.085s 3.054s 11.890s #Disk Read 1.970s 1.996s 2.982s 11.850s <p>It's kind of terrifying how well changing just the wrapper function <code>Cursor.next</code> is, it shows that there is easily optimisation improvements available, that plus the inlining \\(13.9\\)x performance improvement shows that there is room that even if V8 doesn't get around to implementing something, tools like Typescript certainly could.</p> <p>Also if you look at the peaking example, we hit quite an interesting limit. In that case only \\(0.078\\%\\) of requests were fulfilled by the async function, meaning only about \\(9194\\) of \\(11746006\\) requests were waiting for the data to be loaded. This would imply our CPU is almost perfectly being feed by the incoming data.</p>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#conclusion","title":"Conclusion","text":"<p>The performance of asynchronous JavaScript functions can be significantly improved by making simple tweaks to the code. The results of this case study demonstrate the potential for \\(1.9\\)x to \\(14\\)x speed boosts with manual optimizations. The V8's current lack of optimization for these features leaves room for further improvements in the future.</p> <p>When using direct raw <code>Promise</code> API calls, there can be a strong argument made that attempting to optimise this behaviour without potentially altering execution behaviour can be extraordinarily hard to implement. But when we use the <code>async</code>/<code>await</code> syntax without even using the term <code>Promise</code>, our functions are now written in such a way you can make some pretty easy performance guaranteed optimisations.</p> <p>The fact that simply #altering the wrapper call creates an almost \\(1.9\\)x boost in performance should be horrifying for anyone who has used a compiled language. It's a simple function call redirection and can be easily optimised out of existence in most cases.</p> <p>We don't need to wait for the browsers to implement these optimisations, tools such as Typescript already offer transpiling to older ES version, clearly showing the compiler infrastructure has a deep understanding of the behaviour of the language. For a long time people have been saying that Typescript doesn't need to optimise your Javascript, since V8 already does such a good job, however that clearly isn't the case with this new async syntax - and with a little bit of static analysis an inlining alone Javascript can become way more performant.</p>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#take-away","title":"Take Away","text":"<p>Currently in V8's implementation of Javascript, <code>async</code> is just an abstraction of <code>Promise</code>s, and <code>Promise</code>s are just an abstraction of callbacks, and V8 doesn't appear to use the added information that an <code>async</code> function provides over a traditional callback to make any sort of optimisations.</p> <p>While the majority of active async Javascript code is probably IO bounded instead of CPU, this likely won't affect the majority of Javascript code. However your code can still potentially be limited by these performance characteristics even if you're not the one doing the heavy CPU load. Potentially based on how you to interface with a given library could give you massively different performance characteristics depending no if you're using non-synchronous code or not, and the problem can be exacerbated depending on the implementation details of the library.</p>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/async-js-performance-apr23/#what-you-can-do-now","title":"What you can do now","text":"<ol> <li>As a general rule, try and avoid async when possible - and no callbacks are not the solution, because it has the same performance impact.</li> <li>When possible instead of creating a new Promise bounded by another - attempt to merge them into a single Promise when possible.</li> </ol>","tags":["Javascript","Asynchronous","Promise","Performance","Optimisation"]},{"location":"blog/auto-scaling-text-nov23/","title":"CSS Auto Font Scaling","text":"<p>The new CSS Container Queries are actually much more useful than for just switching between different discrete CSS styles. They can actually be easily repurposed for other things such as dynamic font sizing to stop text from clipping off the screen</p>              hi                       because                       pneumonoultramicroscopicsilicovolcanoconiosis          <p>css<pre><code>.scaler {\n    font-size: 14pt; /*fallback*/\n    container-type: inline-size;\n    --char-aspec-ratio: 1.8;\n}\n\n.auto-scaling-font {\n    font-size: min(\n        60px,\n        calc(100cqw / var(--chars) * var(--char-aspec-ratio))\n    );\n}\n</code></pre> html<pre><code>&lt;div class=\"boundary scaler\"&gt;\n    &lt;div class=\"auto-scaling-font\" style=\"--chars: 2\"&gt;\n        hi\n    &lt;/div&gt;\n    &lt;div class=\"auto-scaling-font\" style=\"--chars: 7\"&gt;\n        because\n    &lt;/div&gt;\n    &lt;div class=\"auto-scaling-font\" style=\"--chars: 45\"&gt;\n        pneumonoultramicroscopicsilicovolcanoconiosis\n    &lt;/div&gt;\n&lt;/div&gt;\n</code></pre></p>"},{"location":"blog/database-garbage-collection-dec23/","title":"Implementing Database level garbage collection for Blobs","text":"<p>Something that I often see for media management of online services I use is these two lazy solutions for managing uploaded user media:</p> <ul> <li>File Expiry: WHen a user uploads a file it has a fixed duration until that file disappears, and the app just relies on the fact that hopefully you won't try to load content that is older than the period of time they set.</li> <li>Single File Ownership: If a file is uploaded in a certain post/message or other entity, it's existence is tied to the lifetime of the thing it was originally uploaded to. Meaning if that image was then reused else where, when the original post is deleted all other entities that used that media now have a missing image.</li> <li>Infinite Storage: Services at Facebook/Google/Discord scale often opt for this approach, which is just don't delete anything, because the operating cost of storing media infinitely is less costly than the potential development slow down of actually having garbage collection system.</li> </ul> <p>Ideally want you want is to track where certain media is being used, then when it is no longer being used anywhere, it is then deleted. However this can lead to some instant implementation bottle necks. If all of your posts/comments/etc are all stored in the database, then that means you need to scan the entire database to ensure a certain piece of media is no longer used? No, you can actually exploit some properties of foreign keys to get sweeping for free without room for developer error as the system is maintained.</p> <p>Side Note</p> <p>I'm not so bold or foolish to say the potential solution I am going to propose is perfect or all use cases, that is why I will describe a few variations which could be helpful at different scales, but even then there will still be cases where none of these solutions are optimal for some very specific use cases.</p>","tags":["Database","SQL","Garbage Collection","S3 Storage"]},{"location":"blog/database-garbage-collection-dec23/#naive-approach","title":"Naive Approach","text":"<p>The naive approach would be to make a query which checks anywhere a specific media (aka blob) can be used, and then gather a list of distinct in use blobIDs then from that we then know which media we can delete (everything not in the list). But that leaves a potential massive issue of maintainability.</p> <p>What happens when you add a new entity which uses a blob? If someone forgets to add it into this big query you'll end up with media prematurely being deleted because it's not correctly checked if it's in use.</p> <p>Ideally we want an implementation where it doesn't matter how many new ways you add to use a blob, it can easily be tied into this garbage collector without room for error.</p>","tags":["Database","SQL","Garbage Collection","S3 Storage"]},{"location":"blog/database-garbage-collection-dec23/#referencing-a-blob","title":"Referencing a Blob","text":"<p>The database already has ways to tie different rows together linking them, so it knows which tables might possibly link to a given row, and also indexes which rows link to it. We can leverage this to ensure that a blob can never be deleted when a row in another table still references it.</p> <p>We can do this with a simple <code>ON DELETE RESTRICT</code> foreign key link, we can see in the high level prisma abstraction of our database structure we have our <code>m:n</code> joining table between <code>Blob &lt;-&gt; Post</code> has a cascade on it's deletion of post, and a restrict on the deletion of a blob. This means this joining table will prevent blobs from being deleted when a post references them, but when a post is deleted, this reference is then also deleted.</p> schema.prisma<pre><code>model Blob {\n    id            String @id @default(dbgenerated(\"generate_blob_id()\")) @db.VarChar(11)\n    filename      String @db.VarChar(255)\n    mimetype      String @db.VarChar(255)\n\n    // Computed values\n    visibility VisibilityType @default(PRIVATE)\n    unused     Boolean @default(false)\n    updatedAt DateTime @default(now()) @updatedAt\n}\n\nmodel BlobAttachment {\n    id Int @default(autoincrement()) @id\n\n    blob     Blob   @relation(fields: [blobID], references: [id], onUpdate: Cascade, onDelete: Restrict)\n    blobID   String @db.VarChar(11)\n\n    post      Post?   @relation(fields: [postID], references: [id], onUpdate: Cascade, onDelete: Cascade)\n    postID   String?\n    @@unique([postID, blobID])\n}\n</code></pre>","tags":["Database","SQL","Garbage Collection","S3 Storage"]},{"location":"blog/database-garbage-collection-dec23/#marking-for-deletion","title":"Marking for Deletion","text":"<p>Now we have a method that prevents a blob from being deleted from the database when it's in use, but how can we detect which blobs are able to be deleted so we can then delete them from our actual storage, and then remove them from the database? You try and delete it.</p> <p>We'll start of with the simple case of a single blob, first of all we store the row we want to delete in memory, then we try and delete it, if the delete is successful we reinsert the value back into the database changing <code>unused</code> to <code>true</code>, and if we had an exception trying to deleted it, we can update <code>used</code> to false. We're able to do all of this most importantly in a single SQL query, we didn't need to transfer any information about the blob state across the network.</p> <p>You might also notice there is a bit of extra code in the successful deletion case which checks if we're going to change the <code>unused</code> status, and if so then we also change the <code>updatedAt</code> value to <code>now()</code>, we only do this when the value changes so that we can track how long ago something was marked.</p> Mark Single<pre><code>DO $$\nDECLARE\n  v_blob \"Blob\"%ROWTYPE;\nBEGIN\n  BEGIN\n    -- Store the record into the variable\n    SELECT * INTO v_blob FROM \"Blob\" WHERE id = $1;\n\n    -- Try to delete the record\n    DELETE FROM \"Blob\" WHERE id = $1;\n\n    -- If successful, insert the record back and mark unused as TRUE\n    INSERT INTO \"Blob\" (\"id\", \"filename\", \"mimetype\", \"unused\", \"updatedAt\", \"createdAt\")\n    VALUES (\n      v_blob.\"id\", v_blob.\"filename\", v_blob.\"mimetype\", TRUE,\n      CASE\n        WHEN v_blob.\"unused\" = FALSE THEN now()\n        WHEN v_blob.\"unused\" = TRUE THEN v_blob.\"updatedAt\"\n      END,\n      v_blob.\"createdAt\"\n    );\n\n  EXCEPTION WHEN foreign_key_violation THEN\n    -- If DELETE fails due to foreign key violation, set the unused flag to FALSE\n    UPDATE \"public\".\"Blob\" b\n    SET \"updatedAt\" = CASE\n        WHEN b.\"unused\" = TRUE THEN now()\n        WHEN b.\"unused\" = FALSE THEN b.\"updatedAt\"\n      END,\n      \"unused\" = FALSE\n    WHERE id = $1;\n  END;\nEND;\n$$;\n</code></pre> <p>Now we can scale this up, looping over every row in the <code>Blob</code> table doing the same thing, now we're able to update mark every blob in a table with a single sql query.</p> Mark All<pre><code>DO $$\nDECLARE\n    v_blob \"Blob\"%ROWTYPE;\n    v_blob_cursor CURSOR FOR SELECT * FROM \"Blob\";\nBEGIN\n    -- Open the cursor to iterate through all blobs\n    OPEN v_blob_cursor;\n\n    LOOP\n        -- Fetch next blob record\n        FETCH v_blob_cursor INTO v_blob;\n\n        -- Exit loop if no more blobs\n        EXIT WHEN NOT FOUND;\n\n        BEGIN\n            -- Try to delete the record\n            DELETE FROM \"Blob\" WHERE id = v_blob.id;\n\n            -- If successful, insert the record back and mark unused as TRUE\n            INSERT INTO \"Blob\" (\"id\", \"filename\", \"mimetype\", \"unused\", \"updatedAt\", \"createdAt\")\n            VALUES (\n                v_blob.\"id\", v_blob.\"filename\", v_blob.\"mimetype\", TRUE,\n                CASE\n                    WHEN v_blob.\"unused\" = FALSE THEN now()\n                    WHEN v_blob.\"unused\" = TRUE THEN v_blob.\"updatedAt\"\n                END,\n                v_blob.\"createdAt\"\n            );\n\n        EXCEPTION WHEN foreign_key_violation THEN\n            -- If DELETE fails due to foreign key violation, set the unused flag to FALSE\n            UPDATE \"public\".\"Blob\" b\n            SET \"updatedAt\" = CASE\n                    WHEN \"unused\" = TRUE  THEN now()\n                    WHEN \"unused\" = FALSE THEN b.\"updatedAt\"\n                END,\n                \"unused\" = FALSE\n            WHERE id = v_blob.id;\n        END;\n\n    END LOOP;\n\n    -- Close the cursor\n    CLOSE v_blob_cursor;\nEND;\n$$;\n</code></pre>","tags":["Database","SQL","Garbage Collection","S3 Storage"]},{"location":"blog/database-garbage-collection-dec23/#sweeping","title":"Sweeping","text":"<p>Now that we have marked all of the blobs, we can get a simple list of all items which weren't swept recently. This grace period is important for multiple reasons the biggest one being: a new blob item could have been uploaded but the record linking to it has not yet been saved. Hence we have a grace period to ensure something hasn't gone wrong. Get Sweepable<pre><code>SELECT \"id\"\nFROM \"Blob\"\nWHERE \"unused\" = TRUE\nAND \"updatedAt\" &lt; CURRENT_DATE - INTERVAL '1 days';\n</code></pre></p> <p>Now we have the list of <code>Blob.id</code>s which are ready for deletion and we can use these to delete the actual files from our S3 bucket or equivalent storage. But how were these IDs originally made in such a way that they perfectly make the name they were given in our bucket without conflict?</p>","tags":["Database","SQL","Garbage Collection","S3 Storage"]},{"location":"blog/database-garbage-collection-dec23/#uploading","title":"Uploading","text":"","tags":["Database","SQL","Garbage Collection","S3 Storage"]},{"location":"blog/database-garbage-collection-dec23/#blob-access-in-code","title":"Blob Access in Code","text":"<pre><code>import { BlobPagePoll } from \"./page-using-blob.server\";\nimport { Blob } from \"@prisma/client\";\n\nexport function BlobPath(blobID: Blob['id'], __filename: string, thumbnail: boolean) {\n    if (typeof BlobPagePoll !== 'undefined') BlobPagePoll(blobID, __filename);\n\n    if (thumbnail) return `/blob/${blobID}/thumbnail`;\n    return `/blob/${blobID}/raw`;\n}\n</code></pre> <pre><code>export async function BlobPagePoll(blobID: Blob['id'], url: string) {\n    if (process.env.NODE_ENV !== \"production\") return;\n\n    await limiter.allocate(); // one page update at a time\n\n    await prisma.pageUsingBlob.upsert({\n        where: { blobID_url: { blobID, url } },\n        create: {\n            blobID: blobID,\n            url: url,\n            inUse: true,\n        },\n        update: { inUse: true, }\n    });\n\n    await prisma.blob.update({\n        where: { id: blobID },\n        data: { visibility: \"PUBLIC\" }\n    });\n\n    limiter.free();\n}\n\nexport async function SweepPages() {\n    if (process.env.NODE_ENV !== \"production\") return;\n\n    const past30days = new Date();\n    past30days.setDate(past30days.getDate() - 30);\n    const [ removed, _ ] = await prisma.$transaction([\n        prisma.pageUsingBlob.findMany({\n            where: {\n                inUse: false,\n                updatedAt: { lt: past30days }\n            }\n        }),\n        prisma.pageUsingBlob.deleteMany({\n            where: {\n                inUse: false,\n                updatedAt: { lt: past30days }\n            }\n        })\n    ]);\n\n    await UpdateBlobVisibilities(removed.map(x =&gt; x.blobID));\n}\n</code></pre>","tags":["Database","SQL","Garbage Collection","S3 Storage"]},{"location":"blog/wasm-is-not-going-to-save-javascript-jul23/","title":"Wasm is not going to save Javascript","text":"<p>This article is a case study of the performance impact of improving bnf-parser library to be able to take a given BNF syntax input and compile it all the way down to an optimised parser in wasm for execution - to improve parse times of arbitrary syntaxes.</p>","tags":["Web Assembly","wasm","Javascript","Performance"]},{"location":"blog/wasm-is-not-going-to-save-javascript-jul23/#testing-methodology","title":"Testing Methodology","text":"<p>For each round of testing we will parse two rather complex BNFs (sequalize, lolcode) via three different parsing methods sequentially. Measuring the total parse time for each method using parse hooks. The library itself is actually boot strapped (it compiles and uses itself), and the first stage of compiling a BNF syntax is parsing it - so this is a valid test case for potential parsers generated by the library.</p> <p>All three parse methods are tested per round to keep the execution of each method closely coupled to each other to mitigate the impacts of background processes, and V8 optimisations so that these external factors will hopefully affect each of the parsers similarly.</p> <p>The first two parsers are actually the same wasm compiled parser in two different modes, the first with source mapping enabled, and the second one without. Source mapping is an optional extra parse which can be applied to the wasm parser which correctly maps syntax nodes of the tree to the <code>column</code>, <code>row</code>, and javascript <code>index</code> (don't get me started on UTF-16) it spans. This is an optional extra parse in bnf-parser because it allows the parser to not waste time allocating extra reference objects which aren't necessary for applications which don't need syntax error messages.</p> <p>These compiled parsers have also had the default optimisations applied within binaryen which should hopefully give them an advantage over the Javascript implementation (assuming V8 optimisations don't have their way).</p> <p>The third parser is using bnf-parser's legacy parser which behaves kind of like a graph traversal completely in Javascript where the graph structure is generated from a BNF, and the resulting syntax tree for a given input is generated based on this graph traversal (like a DFA).</p> <p>We ran the testing round <code>10,000</code> consecutive times, gathering results using NodeJS perf_hooks, we then also ran the tests a second time with more in-depth hooks put into the bnf-parser artifacts to see exactly what's going on under the hood. These performance measurements where not taken in the tests comparing the different parsers as the act of measuring them would heavily impact the overall performance of the wasm results, they're just that fast.</p>","tags":["Web Assembly","wasm","Javascript","Performance"]},{"location":"blog/wasm-is-not-going-to-save-javascript-jul23/#results","title":"Results","text":"<p>From the results below we can see a few interesting trends, both the <code>wasm w/ mapping</code> parser and the <code>legacy</code> parse both have significantly higher 99% execution times than their 1%. This is due to the fact both of them are receiving a lot of love from V8's excellent optimiser. For the first couple of runs it's slow, but once the JIT realises it's doing the same thing many times it starts to optimise it.</p> <p>We also did a test where after each testing round we attempted to parse another random non bnf syntax, to see if it threw off V8's optimisations due to the graph traversal functions now running on a different graph to the one it was optimised for. However that had no negligible effect.</p> Wasm <code>w/ map</code> Wasm <code>no map</code> Legacy Max 6.1372ms 2.5623ms 13.5988ms 99% 2.3481ms 0.8897ms 2.2354ms 50% 1.5971ms 0.6350ms 1.6305ms 1% 0.6533ms 0.2202ms 0.4673ms Min 0.6437ms 0.2173ms 0.4602ms Mean 1.2774ms 0.5384ms 1.2102ms <p>Comparing the median <code>legacy</code> times to <code>wasm no map</code> we can see an approximate <code>2.56x</code> - however only <code>legacy</code> is generating <code>SyntaxNode</code> references, so it's not a fair comparison of equivalent compute. Comparing the <code>wasm</code> parser with source mapping to <code>legacy</code> we see only a <code>1.04x</code> improvement.</p> <p>And that might get you thinking - wow, JS really isn't that slow. But comparing <code>wasm</code> to raw JS performance isn't fair either, because you're missing a step. You need to move data in and out of the JS world to the <code>wasm</code> instance, and that has a tax.</p>","tags":["Web Assembly","wasm","Javascript","Performance"]},{"location":"blog/wasm-is-not-going-to-save-javascript-jul23/#the-transport-tax","title":"The transport tax","text":"<p>There are four main stages of using the <code>wasm</code> bnf-parser;</p> <ul> <li>Encoding: This is where you take the input data from Javascript, and write it into the <code>wasm</code> instance's memory, and also tell the instance how long the data you just put in is</li> <li>Parsing: This is the actual work we want to complete, this is iterating over the string and generating the entire syntax tree</li> <li>Decoding: We want to be able to use that tree in JS, so we need to load it back out to be useful - bring it back over to JS land.</li> <li>Mapping: This is generating the source references for a given syntax tree, based on the input data.</li> </ul> <p>It's important to note that the <code>mapping</code> part is almost entirely done in Javascript rather than in <code>wasm</code>, because the computation is super simple, you're just iterating forward over a string counting the index as you depth first traverse over the syntax tree filling in the references (this is done using stack operations, so it's a single function call to save on the extra tax of recursive calls in JS). Since the majority of the complex work being done is simply allocating new objects to store the reference at each point - there will be no real time-saved by doing this in WASM, and any time saved will be mostly lost due to the data transfer tax.</p> Encode Parse Decode Mapping Max 0.2647ms 0.2692ms 2.6253ms 3.5991ms 99% 0.0064ms 0.1443ms 1.0704ms 1.1919ms 50% 0.0026ms 0.1335ms 0.6914ms 0.9063ms 1% 0.0023ms 0.0436ms 0.1738ms 0.4232ms Min 0.0020ms 0.0428ms 0.1720ms 0.4160ms Mean 0.0032ms 0.0910ms 0.5242ms 0.7071ms <p>From this data we can see we are spending <code>40.03%</code> of our time just moving data between JS to WASM land - that's almost half of the entire computation. We can also see the other <code>55.41%</code> is taken up by generating the source references. Leaving only <code>7.70%</code> of the time we took to run this parse actually computing the syntax tree!!!</p>","tags":["Web Assembly","wasm","Javascript","Performance"]},{"location":"blog/wasm-is-not-going-to-save-javascript-jul23/#whats-up-with-javascripts-gst-rates-being-so-high","title":"What's up with Javascript's GST rates being so high?","text":"<p>The reason this tax for transferring data is so high is painfully illustrated by the difference in compute time between <code>decode</code> and <code>mapping</code>. Mapping is much simpler, than trying to traverse a tree generated by a different language where you need to worry about bit alignment, as well as actually decoding the foreign data into something Javascript can use.</p> <p>The reason is object allocation. Everything in Javascript is an object, even the object within an object is an object - and that's objectively a problem.</p> <p>In any statically typed language if you allocate a <code>struct</code> which has another <code>struct</code> as it's member, you get that child for free. That isn't the case in Javascript. Every <code>SyntaxNode</code>, has a <code>ReferenceRange</code> which contains two <code>Reference</code> objects - so that means if you want to allocate a <code>SyntaxNode</code> and fill in all of it's children, that's actually <code>4</code> allocations, not <code>1</code>.</p> <p>The reason decoding is able to be as fast as it is; is because of object reuse. By default every single <code>SyntaxNode</code> actually shares the same <code>ReferenceRange</code> instance, that means that range and it's two children only need to be allocated once, and every <code>SyntaxNode</code> gets a <code>ReferenceRange</code> so now you don't need to do null checks everywhere - and we only have one allocation per node.</p> <p>But when you run the source map over the syntax tree, now for every single <code>SyntaxNode</code> you have to perform <code>3</code> allocations: <code>ReferenceRange</code>, start <code>Reference</code>, and end <code>Reference</code>.</p> <p>Part of the reason the execution in <code>wasm</code> is actually so fast is because it only does one allocation, the entire tree itself. The entire tree represented in <code>wasm</code> is actually flat packed into linear memory. And since after every parse the data is read out, we don't need the previous tree after each parse - so we can just write over it. So we have zero allocations because we just use the same single allocation. In other languages line <code>C++</code> you could allocate a vector a factor or two larger than your estimated tree size, then compute your flat tree, then shrink afterwards. Two allocations.</p> <p>In Javascript everything is an object, everything must be independently allocated.</p>","tags":["Web Assembly","wasm","Javascript","Performance"]},{"location":"blog/wasm-is-not-going-to-save-javascript-jul23/#can-wasm-still-work-as-an-accelerator","title":"Can Wasm still work as an accelerator?","text":"<p>Wasm libraries can still work as an accelerator to Javascript, in almost an identical way to how everything in Python is actually a C library. You could have a library for matrix multiplication, and all of your matrices are permanently stored in WASM, only coming out after computation is complete to be printed, sent over the network, or written to file.</p> <p>So much like the current Python eco system, JS could lead towards a world where it's a glue language - the problem is that for typical Javascript workflows it's <code>90%</code> glue.</p> <p>For the vast majority of Javascript execution it's:</p> <ul> <li>Take something from the network</li> <li>Perform a small amount of manipulation</li> <li>Send it out to the network, or write to the DOM.</li> </ul> <p>JS is primarily a middle man language, attempting to use it like python to abstract the middle man's duty to another person creating another middle man leads to very little performance gain, and a whole lot of headache. Try talking to the tech support of any major tech company and you'll see what I mean.</p> <p>Wasm also has another extra headache. It's security focused, meaning every wasm instance has it's own independent memory, that means two different wasm libraries can't actually share memory, unless they are recompiled together, or you parse data between wasm libraries much like you do from JS to wasm.</p> <p>Plus the majority of wasm compiled modules don't actually play nicely together, they're compiled to rule their sandpit, and no one else can enter. If you attempted to bring a C++ library and Rust library into the same WASM module, who's <code>malloc</code> implementation are we using? There is only one linear memory, and they can't both be operating on the same space. Who's are we choosing? How do we choose? How do each of the children know who was chosen?</p>","tags":["Web Assembly","wasm","Javascript","Performance"]},{"location":"blog/wasm-is-not-going-to-save-javascript-jul23/#wasm-is-what-people-wanted-docker-to-be","title":"Wasm is what people wanted docker to be","text":"<p>Wasm is a really powerful tool, but I think people are miss understanding where it's heading and what it will be great for. No it will not be great for bringing that one Rust library to use in your TS workflow. It's better to think of it like a super light weight and actually portable docker container that can execute anywhere.</p> <p>You can bring that container into the browser to act as your front end, or you can have it running as micro-service, or the entire backend. What it's not is a way to use language <code>X</code>'s library in language <code>Y</code>.</p>","tags":["Web Assembly","wasm","Javascript","Performance"]},{"location":"blog/the-upper-limit-of-wasm-performance/","title":"The Upper Limits of WebAssembly Net Performance","text":"<p>Wasmer.io recently released an article announcing their Winter.js 1.0, however looking at the details of their benchmarks it shows that running Winter.js in wasm results in a 12x slow down in performance compared to native.</p> <p>That large a performance difference just doesn't sit right with me based on my experience with wasm, yes it should be slower, I would believe 4x, but 12x!?!?!? What is this, a PC port of a console game?</p> <p>Looking at the code bases of most current software with wasm support, if you get to the underlying implementation - it's a lot closer to a software port, than a new target assembly. I would be willing to bet that over the coming years you will all sorts of 2x 4x 10x articles, about how software X massively improved their wasm performance since they started putting dev hours into it because the technology became relevant.</p> <p>But I want to know what is the upper limit of wasm, what will be those future performance gains? I don't have a crystal ball, but I do have two smooth rocks in my back-yard and an IDE, so let's hand roll some web assembly and find out.</p>","tags":["web assembly","performance","http"]},{"location":"blog/the-upper-limit-of-wasm-performance/#banging-some-rocks","title":"Banging Some Rocks","text":"<p>First of all we're going to need to bind some OS functions, so we can talk to the OS's network layer. This is done via functions very similar to the POSIX standard called WASIX. They get imported in kind of a similar way to a DLL, where you specify which functions you're trying to load and from where.</p> <p>But how do you know where to import these functions? Idk bro, I just grepped the wasix-libc till I found something that looked right.</p> <p>There is another important bit to note, Web Assembly is designed for the web, and thus to be sent over the network, so the binaries are designed to be very small. So there is a lot of features to reduce the number of bytes in a binary such as LEB128 integer encoding. But more importantly for us, it means that function signatures are declared separately to function bodies so they can be reused. So you end up with something kind of cursed like this. Import Section<pre><code>(type (;0;) (func (param i32) (result i32)))\n(type (;1;) (func (param i32 i32) (result i32)))\n(type (;2;) (func (param i32 i32 i32) (result i32)))\n(type (;3;) (func (param i32 i32 i32 i32) (result i32)))\n(type (;4;) (func (param i32 i32 i32 i32 i32) (result i32)))\n(type (;5;) (func))\n(type (;6;) (func (param i32)))\n(type (;7;) (func (param i32 i32)))\n(import \"wasix_32v1\" \"fd_write\" (func (;0;) (type 3)))\n(import \"wasix_32v1\" \"fd_close\" (func (;1;) (type 0)))\n(import \"wasix_32v1\" \"sock_open\"      (func (;2;) (type 3)))\n(import \"wasix_32v1\" \"sock_bind\"      (func (;3;) (type 1)))\n(import \"wasix_32v1\" \"sock_listen\"    (func (;4;) (type 1)))\n(import \"wasix_32v1\" \"sock_accept_v2\" (func (;5;) (type 3)))\n(import \"wasix_32v1\" \"sock_send\"      (func (;6;) (type 4)))\n(import \"wasix_32v1\" \"sock_status\"    (func (;7;) (type 1)))\n(import \"wasix_32v1\" \"proc_exit\"      (func (;8;) (type 6)))\n(import \"wasix_32v1\" \"sock_shutdown\"  (func (;9;) (type 1)))\n</code></pre></p> <p>Now we have an outline of all of the functions we're going to use, let's quickly map out the lifetime of our program. We're really trying to just test the performance of the WASIX network stack, so doing anything to funky with multithreading or advanced algorithms would be more a test of current runtime multithreaded implementation than the root performance drops that might never be removable from the networking interface.</p> <p>So we want a really hot single threaded loop, that means blocking, but we want to only block in times our CPU couldn't be doing something else anyway. We also literally don't care anything about what the incoming request says, because we're testing raw TCP throughput request.</p> <pre><code>graph TD;\n  Start --&gt; sock_open[\"Create OS socket\"]\n  sock_open --&gt; sock_bind[\"Tell OS our plans for the socket\"]\n  sock_bind --&gt; sock_listen[\"Tell OS we want incoming connections, and to buffer them\"]\n  sock_listen --&gt; sock_accept_v2[\"Wait for a request to arrive\"]\n  sock_accept_v2 --&gt; sock_send[\"Send message to request\"]\n  sock_send --&gt; sock_shutdown[\"Close incoming request\"]\n  sock_shutdown --&gt; sock_accept_v2</code></pre> <p>Our whole program is pretty much setup, then a loop with three functions in it can go around blazingly fast.</p> <p>First of all let's get our data out of the way</p> <ul> <li>Opening on the same port for incoming requests</li> <li>We're always replying with the same message</li> <li>Only Responding to one request at a time</li> </ul> <p>This means all of this memory can be predefined at compile time to be reused.</p> <p>So we'll make our struct defining what we're listening for: sockaddr_in<pre><code>(data (i32.const 48) \"\\01\\00\")                   ;; sin_family: AF_INET = 0x0001\n(data (i32.const 50) \"\\90\\1f\")                   ;; sin_port:      8080 = 0x1F90\n(data (i32.const 52) \"\\00\\00\\00\\00\")             ;; sin_addr:INADDR_ANY = 0.0.0.0\n(data (i32.const 56) \"\\00\\00\\00\\00\\00\\00\\00\\00\") ;; sin_zero = char[8] padding for sockaddr compatibility\n</code></pre></p> <p>Now we'll craft our output response - this is encoded with an <code>iovec</code> which is basically just two <code>i32</code> integers slapped together, the first is a pointer to the start of the buffer, and the second being the length of the buffer. HTTP Response Data<pre><code>(data (i32.const 80) \"\\58\\00\\00\\00\\24\\00\\00\\00\")\n(data (i32.const 88) \"HTTP/1.1 200 OK\\0d\\0a\\0d\\0aHello, World!\")\n</code></pre></p> <p>When we get an incoming request we need a place to store it's details so we can tell the OS which request we're responding to. remote sock_addr<pre><code>(data (i32.const 160) \"\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\")\n\n;; stack: offset.255\n</code></pre></p> <p>Imports done, global variables done, now we just need the actual code.</p> <p>First we need to define our function, and the fact it has two local variables, these will be used to store the file descriptor for the socket we have open, and the file descriptor socket of an incoming request. These local variables are a lot closer to user defined registers than actual variables. <pre><code>(func (;10;) (type 5) (local i32 i32)\n  ;; ...\n)\n</code></pre></p> <p>Now we create a new OS socket, specifying it's an IPv4 socket (<code>AF_INET</code>), and that it's using TCP (<code>SOCK_STREAM</code>), since the early specification of wasm doesn't allow multiple return, the return value from this first call is an error code - but we don't care about that. We give it a pointer to <code>255</code> which is a region that won't interfere with our global data, which after a successful call the file descriptor will be written there, then we load it to a local variable. <pre><code>;; Create socket using sock_open\ni32.const 1    ;; AF_NET\ni32.const 1    ;; SOCK_STREAM\ni32.const 0    ;; Protocol\ni32.const 255  ;; result pointer\ncall 2         ;; sock_open()\ndrop           ;; we don't care about errors\n\n;; Load the socket descriptor from memory\ni32.const 255\ni32.load\nlocal.set 0\n</code></pre></p> <p>Next step, bind the socket to the <code>sockaddr_in</code> we defined earlier in global memory <pre><code>;; Bind socket to address and port\nlocal.get 0   ;; Socket file descriptor\ni32.const 48  ;; Address of the sockaddr_in structure\ncall 3        ;; sock_bind()\ndrop          ;; if it's not going to error, hopefully\n</code></pre></p> <p>Tell the OS we're listening for requests, and queue up to 100 pending connections <pre><code>;; Listen for incoming connections\nlocal.get 0     ;; Socket file descriptor\ni32.const 100   ;; Backlog (maximum pending connections)\ncall 4          ;; sock_listen()\ndrop            ;; it's just wasted cycles\n</code></pre></p> <p>Now for the hot loop <pre><code>(loop\n  local.get 0    ;; Listening socket file descriptor\n  i32.const 0    ;; Desired file descriptor flags (default)\n  i32.const 64   ;; result pointer: new socket\n  i32.const 160  ;; result pointer: remote address\n  call 5         ;; sock_accept_v2()\n  drop           ;; we only accept winners in these parts\n\n  ;; Load the new socket descriptor from memory\n  i32.const 64\n  i32.load\n  local.set 1\n\n  ;; Send response to the client\n  local.get 1    ;; socket\n  i32.const 80   ;; iovs\n  i32.const 1    ;; iovs_len\n  i32.const 0    ;; No additional flags\n  i32.const 160  ;; ptr: remote address\n  call 6         ;; sock_send()\n  drop           ;; get dropped\n\n  ;; Shutdown the socket\n  local.get 1 ;; socket\n  i32.const 2 ;; how: SHUT_RDWR\n  call 9      ;; sock_shutdown()\n  drop        ;; we're done here\n\n  ;; Close the fd\n  local.get 1 ;; socket\n  call 1      ;; fd_close()\n  drop        ;; bye\n\n  br 0\n)\n</code></pre></p>","tags":["web assembly","performance","http"]},{"location":"blog/the-upper-limit-of-wasm-performance/#testing-methodology","title":"Testing Methodology","text":"<p>For testing we want something directly comparable with Winter.js's benchmark so we used Wrk which is made for linux systems. So a linux system we shall go.</p> <p>Dual booting with modern Windows + secure boot + TPMs make life painful, so my system doesn't run native linux. A VPS could have noisy neighbours which will skew results so we can't use one of those. I had issues compiling some of this stuff for ARM, so Raspberry Pi 4 is out the window. So I used wsl, which definitely hurt performance - but it will hurt everyone's performance equally so that's okay.   </p> <p>I ran the <code>server.wat</code> through <code>wasmer direct</code>, as well as using <code>wasmer create-exe --llvm</code> to get the highest web assembly performance possible. However Winter.js giving the same treatment to it's wasm port caused a compilation error I'd need a full time job to debug.</p> <p>I rewrote the <code>server.wat</code> in C to make <code>server.c</code> as an apples to apples native comparison.</p> <p>I also ran Winter.js's NodeJS and Bun benchmarks to have a shared point of reference.</p> <p>For each test I ran it three times, taking the median <code>req/sec avg</code> value for the graph below.</p>","tags":["web assembly","performance","http"]},{"location":"blog/the-upper-limit-of-wasm-performance/#results","title":"Results","text":"<pre><code>wrk -t12 -c400 -d10s http://127.0.0.1:8080\n</code></pre> <p>*I was unable to get Winter.js to compile, so the value on this graph is an estimate based on it's relative performance to <code>Bun</code>, <code>Node</code> and <code>Winter.js (WASIX)</code>. For exact details you can see the spreadsheet here</p> <p>Initially looking at these results the Bun and Winter.js seem super sus if we assume they were single threaded, since the underlying Javascript should be executing on a single thread (this is also why I didn't test Go).</p> <p>If we think about our hot loop flow path, at what times are we waiting when we could be executing the response (limited to a single thread)? The only time the listen blocks, is when there are no pending requests because we're waiting for the next request. And when there is a request waiting the function should instantly return.</p> <p>When we send data down the socket, there is no waiting their either, because the OS wait for confirmation a single packet is received before sending the next one away. Shutting down the socket and closing the file descriptor would trigger OS level cleaning of those utilise which would also not cause a wait. Assuming all of these are handled well by the OS there shouldn't be much of a wait we're only sending and receiving tens of bytes.</p>","tags":["web assembly","performance","http"]},{"location":"blog/the-upper-limit-of-wasm-performance/#so-lets-look-at-the-syscalls","title":"So Let's Look at the Syscalls","text":"<p>So I ran the <code>server.wat</code> again, this time with tracing, and then I manually removed the first couple of lines to the logs start when it listens for it's second ever request. Since the first request will have a long blocking period, because I haven't started the <code>wrk</code> command yet. RUST_LOG=wasmer_wasix=trace wasmer run server.wat --net &amp;&gt; log.txt<pre><code>sock_accept_v2: wasmer_wasix::syscalls::wasix::sock_accept: return=Ok(Errno::success) sock=6 fd=9614\nsock_accept_v2: wasmer_wasix::syscalls::wasix::sock_accept: close time.busy=137\u00b5s time.idle=1.01\u00b5s sock=6 fd=9614\nsock_send: wasmer_wasix::syscalls::wasix::sock_send: bytes_written=36 fd=9614\nsock_send: wasmer_wasix::syscalls::wasix::sock_send: return=Ok(Errno::success) fd=9614 nsent=36\nsock_send: wasmer_wasix::syscalls::wasix::sock_send: close time.busy=224\u00b5s time.idle=842ns fd=9614 nsent=36\nsock_shutdown: wasmer_wasix::syscalls::wasix::sock_shutdown: return=Ok(Errno::notconn) sock=9614\nsock_shutdown: wasmer_wasix::syscalls::wasix::sock_shutdown: close time.busy=91.6\u00b5s time.idle=781ns sock=9614\nfd_close: wasmer_wasix::fs: closing file descriptor fd=9614 inode=9615 ref_cnt=1 pid=1 fd=9614\nfd_close: wasmer_wasix::syscalls::wasi::fd_close: return=Ok(Errno::success) pid=1 fd=9614\nfd_close: wasmer_wasix::syscalls::wasi::fd_close: close time.busy=191\u00b5s time.idle=852ns pid=1 fd=9614\n</code></pre></p> <p>Now we'll do a little bit of python to get the aggregate values, since our hot loop is really tight, it doesn't matter if we sum or mean our <code>time</code>s per call, because each call is made once per iteration.</p> <p></p> <p>From this it's obvious that our assumption was partly correct, <code>sock_shutdown</code> does basically nothing, same with <code>sock_accept_v2</code> since we have constant incoming requests, but there are two big problems, the <code>fd_close</code> and <code>sock_send</code>.  </p> <p><code>fd_close</code> presumably runs all of the necessary OS cleanup on the file descriptor then and there before switching context back to our app, and this is also likely the same for <code>sock_send</code> since in comparison to most system calls, they're very cheap. The problems is that since we're only making cheap alls, to us they're quite expensive - and this is where Winter.js and Bun can run ahead.</p> <p>Depending on what mechanism you use to communicate between threads in a program, it can be cheaper than a system call. Hence if instead of doing the expensive <code>sock_send</code>, <code>sock_shutdown</code> and <code>fd_close</code> on our main thread we just throw them over to a secondary slave thread to do our dirty-laundry we could actually seem measurable performance increases. Which is likely the main reason why Winter.js and Bun can pull ahead - because they're likely both doing this.</p> <p>This is also likely the reason why Winter.js in wasm is super slow, because the multithreaded model in Web Assembly might not be highly optimised, hence the communication between threads could end up being more costly that just running the system call. This would get us the exact results we saw in our first graph.</p>","tags":["web assembly","performance","http"]},{"location":"blog/the-upper-limit-of-wasm-performance/#summary","title":"Summary","text":"<p>Just like I said in the beginning there is a big chance that current web assembly performance will increase at the programming language level, I think there is still room for improvement based on these graphs. Web Assembly didn't start with a multithreading specification, it was added later and is still a flag you have to enable on some runtimes, so it makes sense that it might not be well optimised yet. This is then likely compounded by the fact no programming language is probably using the existing multithreaded systems properly, so the optimisation focused is more on the languages rather than the runtimes.</p> <p>I don't think Web Assembly will ever reach the performance of native, but that's not the point, all it needs to be is on par with the performance of current virtualisation platforms. Based on the fact that we can already touch Node performance, the currently available runtimes are suitable for a lot of current server workloads - the question is if it can get to the point where it's applicable for all server work loads. Where you can just push your complete server bundled as single wasm binary, specify a few environment variables and let the data centers handle it from there.</p> <p>Source code for benchmarks and raw results can be found in the Appendix</p>","tags":["web assembly","performance","http"]},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/","title":"Appendix: Implementing Database level garbage collection for Blobs","text":"<p>Article</p>"},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/#benchmark-source","title":"Benchmark Source","text":"server.c<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/socket.h&gt;\n#include &lt;netinet/in.h&gt;\n\n#define BACKLOG 100\n\nvoid panic(int code, int lineNum) {\n    if (code == 0) {\n        return;\n    }\n\n    char error_msg[64];\n    snprintf(error_msg, sizeof(error_msg), \"Panic %d @ %d\\n\", code, lineNum);\n    __attribute__((unused)) ssize_t bytes_written = write(STDOUT_FILENO, error_msg, strlen(error_msg));\n    exit(code);\n}\n\nint main() {\n    int server_fd, new_socket;\n    struct sockaddr_in address;\n    int addrlen = sizeof(address);\n\n    // Create socket\n    if ((server_fd = socket(AF_INET, SOCK_STREAM, 0)) == 0) {\n        panic(255, 31);\n    }\n\n    address.sin_family = AF_INET;\n    address.sin_addr.s_addr = INADDR_ANY;\n    address.sin_port = htons(8080);\n\n    // Bind socket to address and port\n    if (bind(server_fd, (struct sockaddr *)&amp;address, sizeof(address)) &lt; 0) {\n        panic(1, 44);\n    }\n\n    // Listen for incoming connections\n    if (listen(server_fd, BACKLOG) &lt; 0) {\n        panic(1, 56);\n    }\n\n    while (1) {\n        // Accept incoming connection\n        if ((new_socket = accept(server_fd, (struct sockaddr *)&amp;address, (socklen_t*)&amp;addrlen)) &lt; 0) {\n            panic(1, 69);\n        }\n\n        // Send HTTP response\n        const char *response = \"HTTP/1.1 200 OK\\r\\n\\r\\nHello, World!\";\n        ssize_t send_result = send(new_socket, response, strlen(response), 0);\n        if (send_result &lt; 0) {\n            panic(1, 93);\n        }\n\n        // Shutdown and close the socket\n        shutdown(new_socket, SHUT_RDWR);\n        close(new_socket);\n    }\n\n    return 0;\n}\n</code></pre> server.wat<pre><code>(module\n    (type (;0;) (func (param i32) (result i32)))\n    (type (;1;) (func (param i32 i32) (result i32)))\n    (type (;2;) (func (param i32 i32 i32) (result i32)))\n    (type (;3;) (func (param i32 i32 i32 i32) (result i32)))\n    (type (;4;) (func (param i32 i32 i32 i32 i32) (result i32)))\n    (type (;5;) (func))\n    (type (;6;) (func (param i32)))\n    (type (;7;) (func (param i32 i32)))\n    (import \"wasix_32v1\" \"fd_write\" (func (;0;) (type 3)))\n    (import \"wasix_32v1\" \"fd_close\" (func (;1;) (type 0)))\n    (import \"wasix_32v1\" \"sock_open\"      (func (;2;) (type 3)))\n    (import \"wasix_32v1\" \"sock_bind\"      (func (;3;) (type 1)))\n    (import \"wasix_32v1\" \"sock_listen\"    (func (;4;) (type 1)))\n    (import \"wasix_32v1\" \"sock_accept_v2\" (func (;5;) (type 3)))\n    (import \"wasix_32v1\" \"sock_send\"      (func (;6;) (type 4)))\n    (import \"wasix_32v1\" \"sock_status\"    (func (;7;) (type 1)))\n    (import \"wasix_32v1\" \"proc_exit\"      (func (;8;) (type 6)))\n    (import \"wasix_32v1\" \"sock_shutdown\"  (func (;9;) (type 1)))\n\n    ;; main()\n    (func (;10;) (type 5) (local i32 i32)\n        ;; Create socket using sock_open\n        i32.const 1    ;; PF_NET\n        i32.const 1    ;; SOCK_STREAM\n        i32.const 0    ;; Protocol\n        i32.const 255  ;; result pointer\n        call 2         ;; sock_open()\n\n        ;; panic on error @\n        i32.const 31\n        call 11\n\n        ;; Load the socket descriptor from memory\n        i32.const 255\n        i32.load\n        local.set 0\n\n        ;; Bind socket to address and port\n        local.get 0   ;; Socket file descriptor\n        i32.const 48  ;; Address of the sockaddr_in structure\n        call 3        ;; sock_bind()\n\n        ;; panic on error @\n        i32.const 44\n        call 11\n\n        local.get 0\n        call 13 ;; sock_status_print()\n        drop\n\n        ;; Listen for incoming connections\n        local.get 0     ;; Socket file descriptor\n        i32.const 100   ;; Backlog (maximum pending connections)\n        call 4          ;; sock_listen()\n\n        ;; panic on error @\n        i32.const 56\n        call 11\n\n        local.get 0\n        call 13 ;; sock_status_print()\n        drop\n\n        (loop\n            local.get 0    ;; Listening socket file descriptor\n            i32.const 0    ;; Desired file descriptor flags (default)\n            i32.const 64   ;; result pointer: new socket\n            i32.const 160  ;; result pointer: remote address\n            call 5         ;; sock_accept_v2()\n\n            ;; panic on error @\n            i32.const 69\n            call 11\n\n            ;; Load the new socket descriptor from memory\n            i32.const 64\n            i32.load\n            local.set 1\n\n            ;; Send response to the client\n            local.get 1    ;; socket\n            i32.const 80   ;; iovs\n            i32.const 1    ;; iovs_len\n            i32.const 0    ;; No additional flags\n            i32.const 160  ;; ptr: remote address\n            call 6         ;; sock_send()\n\n            ;; panic on error @\n            i32.const 93\n            call 11\n\n            ;; Shutdown the socket\n            local.get 1 ;; socket\n            i32.const 2 ;; how: SHUT_RDWR\n            call 9      ;; sock_shutdown()\n            drop\n\n            ;; Close the fd\n            local.get 1 ;; socket\n            call 1      ;; fd_close()\n            drop\n\n            br 0\n        )\n    )\n\n    ;; panicOnError(code: i32, lineNum: i32)\n    (func (;11;) (type 7) (param i32 i32)\n        local.get 0\n        i32.const 0\n        i32.eq\n        if\n            return\n        end\n\n        ;; overwrite string encoding param.0\n        i32.const 14\n        local.get 0\n        i32.const 100\n        i32.div_u\n        i32.const 2\n        call 12\n        i32.store8\n\n        i32.const 15\n        local.get 0\n        i32.const 10\n        i32.div_u\n        i32.const 1\n        call 12\n        i32.store8\n\n        i32.const 16\n        local.get 0\n        i32.const 0\n        call 12\n        i32.store8\n\n        ;; overwrite string encoding param.1\n        i32.const 20\n        local.get 1\n        i32.const 100\n        i32.div_u\n        i32.const 2\n        call 12\n        i32.store8\n\n        i32.const 21\n        local.get 1\n        i32.const 10\n        i32.div_u\n        i32.const 1\n        call 12\n        i32.store8\n\n        i32.const 22\n        local.get 1\n        i32.const 0\n        call 12\n        i32.store8\n\n        ;; write to buffer\n        i32.const 1   ;; std.io file descriptor\n        i32.const 0   ;; iovs\n        i32.const 1   ;; iovs_len\n        i32.const 255 ;; nwritten\n        call 0        ;; fd_write\n        drop\n\n        local.get 0\n        call 8\n    )\n\n    ;; digitToChar(num: i32, place: i32)\n    (func (;12;) (type 1) (param i32 i32) (result i32) (local i32)\n        local.get 0\n        i32.const 10\n        i32.rem_u\n        local.set 2\n\n        local.get 1\n        i32.const 0\n        i32.ne\n        if\n            local.get 2\n            i32.const 0\n            i32.eq\n            if\n                i32.const 95\n                return\n            end\n        end\n\n        local.get 2\n        i32.const 48\n        i32.add\n        return\n    )\n\n    ;; sock_status_print(fd: i32)\n    (func (;13;) (type 0) (param i32) (result i32) (local i32)\n        local.get 0    ;; socket\n        i32.const 255  ;; ptr: status\n        call 7         ;; sock_status()\n\n        ;; panic on error @\n        i32.const 185\n        call 11\n\n        i32.const 255\n        i32.load\n        local.set 1\n\n        ;; overwrite string encoding\n        i32.const 145\n        local.get 1\n        i32.const 100\n        i32.div_u\n        i32.const 2\n        call 12\n        i32.store8\n\n        i32.const 146\n        local.get 1\n        i32.const 10\n        i32.div_u\n        i32.const 1\n        call 12\n        i32.store8\n\n        i32.const 147\n        local.get 1\n        i32.const 0\n        call 12\n        i32.store8\n\n        ;; write to buffer\n        i32.const 1    ;; std.io file descriptor\n        i32.const 124  ;; iovs\n        i32.const 1    ;; iovs_len\n        i32.const 255  ;; nwritten\n        call 0         ;; fd_write\n        drop\n\n        local.get 1\n        return\n    )\n\n    ;; sock_wait_opened(fd: i32)\n    (func (;14;) (type 6) (param i32)\n        block\n            loop\n                local.get 0\n                call 13 ;; sock_status_print()\n\n                i32.const 0\n                i32.ne\n                if\n                    br 2\n                end\n                br 0\n            end\n        end\n    )\n\n    (memory (;0;) 1)\n    (export \"memory\" (memory 0))\n    (export \"_start\" (func 10))\n\n    (data (i32.const 0) \"\\08\\00\\00\\00\\10\\00\\00\\00\")\n    (data (i32.const 8) \"Panic ??? @ ???\\0a\")\n\n    ;; sockaddr_in\n    (data (i32.const 48) \"\\01\\00\")                   ;; sin_family: AF_INET = 0x0001\n    (data (i32.const 50) \"\\90\\1f\")                   ;; sin_port:      8080 = 0x1F90\n    (data (i32.const 52) \"\\00\\00\\00\\00\")             ;; sin_addr:INADDR_ANY = 0.0.0.0\n    (data (i32.const 56) \"\\00\\00\\00\\00\\00\\00\\00\\00\") ;; sin_zero = char[8] padding for sockaddr compatibility\n\n    ;; http response\n    (data (i32.const 80) \"\\58\\00\\00\\00\\24\\00\\00\\00\")\n    (data (i32.const 88) \"HTTP/1.1 200 OK\\0d\\0a\\0d\\0aHello, World!\")\n\n    (data (i32.const 124) \"\\84\\00\\00\\00\\14\\00\\00\\00\")\n    (data (i32.const 132) \"Sock Status: ???\\0d\\0a\")\n\n    ;; global: remote sock_addr\n    (data (i32.const 160) \"\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\")\n\n    ;; stack: offset.255\n)\n</code></pre> <p>Other Benchmarks</p>"},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/#raw-results","title":"Raw Results","text":""},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/#webassembly-wasmer-runtime","title":"WebAssembly (wasmer runtime)","text":"<pre><code>wasmer server.wasm --net\n</code></pre> <pre><code>Running 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    26.55ms  128.64ms   1.77s    95.69%\n    Req/Sec     1.99k     1.30k   29.42k    83.84%\n  236457 requests in 10.10s, 8.12MB read\n  Socket errors: connect 0, read 236457, write 0, timeout 37\nRequests/sec:  23413.55\nTransfer/sec:    823.16KB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    10.13ms   73.05ms   1.76s    98.36%\n    Req/Sec     1.92k     1.18k    6.46k    65.69%\n  226805 requests in 10.07s, 7.79MB read\n  Socket errors: connect 0, read 226805, write 0, timeout 12\nRequests/sec:  22533.68\nTransfer/sec:    792.23KB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    13.71ms   83.90ms   1.77s    97.23%\n    Req/Sec     2.03k     1.34k   18.36k    72.21%\n  236314 requests in 10.10s, 8.11MB read\n  Socket errors: connect 0, read 236315, write 0, timeout 19\nRequests/sec:  23397.43\nTransfer/sec:    822.60KB\n</code></pre>"},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/#webassembly-wasmer-llvm","title":"WebAssembly (wasmer llvm)","text":"<pre><code>wat2wasm server.wat -o server.wasm\nwasmer create-exe --llvm server.wasm -o server.out\n</code></pre> <pre><code>Running 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    20.35ms  117.18ms   1.77s    96.60%\n    Req/Sec     2.51k     1.77k   34.89k    80.43%\n  299600 requests in 10.10s, 10.29MB read\n  Socket errors: connect 0, read 299600, write 0, timeout 39\nRequests/sec:  29665.16\nTransfer/sec:      1.02MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    17.29ms  106.60ms   1.76s    97.06%\n    Req/Sec     2.50k     1.29k   14.02k    77.21%\n  301093 requests in 10.10s, 10.34MB read\n  Socket errors: connect 0, read 301093, write 0, timeout 23\nRequests/sec:  29811.75\nTransfer/sec:      1.02MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    22.95ms  123.38ms   1.77s    96.22%\n    Req/Sec     2.47k     1.64k   36.88k    84.55%\n  296337 requests in 10.10s, 10.17MB read\n  Socket errors: connect 0, read 296337, write 0, timeout 30\nRequests/sec:  29339.53\nTransfer/sec:      1.01MB\n</code></pre>"},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/#winterjs-wasmer-runtime","title":"Winter.js (wasmer runtime)","text":"<pre><code>Running 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    55.14ms    8.91ms 165.95ms   93.45%\n    Req/Sec   396.56    138.06   680.00     62.94%\n  47648 requests in 10.05s, 5.47MB read\nRequests/sec:   4740.14\nTransfer/sec:    556.95KB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    50.93ms    6.07ms 215.99ms   84.28%\n    Req/Sec   200.83    106.82   686.00     68.49%\n  24136 requests in 10.04s, 2.77MB read\nRequests/sec:   2403.87\nTransfer/sec:    282.71KB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    52.97ms    9.61ms 251.05ms   90.47%\n    Req/Sec   112.27     79.62   400.00     65.18%\n  13092 requests in 10.04s, 1.50MB read\nRequests/sec:   1304.06\nTransfer/sec:    153.45KB\n</code></pre>"},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/#node","title":"Node","text":"<pre><code>Running 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    45.82ms  177.25ms   1.99s    95.62%\n    Req/Sec     2.88k   835.90    11.15k    90.74%\n  325385 requests in 10.03s, 53.37MB read\n  Socket errors: connect 0, read 0, write 0, timeout 43\nRequests/sec:  32447.16\nTransfer/sec:      5.32MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    45.24ms  171.77ms   2.00s    95.86%\n    Req/Sec     2.49k   816.56    11.11k    90.42%\n  280164 requests in 10.06s, 45.96MB read\n  Socket errors: connect 0, read 0, write 0, timeout 64\nRequests/sec:  27851.09\nTransfer/sec:      4.57MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    43.95ms  168.63ms   1.99s    95.99%\n    Req/Sec     2.51k     0.88k   17.13k    90.40%\n  283702 requests in 10.06s, 46.54MB read\n  Socket errors: connect 0, read 0, write 0, timeout 60\nRequests/sec:  28193.25\nTransfer/sec:      4.62MB\n</code></pre>"},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/#bun","title":"Bun","text":"<pre><code>Running 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     3.21ms    1.11ms  40.54ms   97.62%\n    Req/Sec    10.44k   699.00    16.63k    85.83%\n  1254427 requests in 10.06s, 143.56MB read\nRequests/sec: 124727.81\nTransfer/sec:     14.27MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     3.48ms  362.15us  11.16ms   80.94%\n    Req/Sec     9.52k   506.77    18.89k    94.37%\n  1143770 requests in 10.06s, 130.89MB read\nRequests/sec: 113749.38\nTransfer/sec:     13.02MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     3.14ms  382.54us   8.30ms   84.57%\n    Req/Sec    10.57k   786.85    20.00k    97.35%\n  1268543 requests in 10.02s, 145.17MB read\nRequests/sec: 126545.76\nTransfer/sec:     14.48MB\n</code></pre>"},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/#c-gcc-o0","title":"C (gcc -O0)","text":"<pre><code>Running 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    21.26ms  117.42ms   1.77s    95.95%\n    Req/Sec     4.73k     2.65k   16.46k    72.86%\n  562425 requests in 10.10s, 17.16MB read\n  Socket errors: connect 0, read 562425, write 0, timeout 36\nRequests/sec:  55685.12\nTransfer/sec:      1.70MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    27.25ms  143.89ms   1.77s    95.87%\n    Req/Sec     4.85k     2.46k   17.69k    66.83%\n  579112 requests in 10.10s, 17.67MB read\n  Socket errors: connect 0, read 579112, write 0, timeout 35\nRequests/sec:  57339.04\nTransfer/sec:      1.75MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    26.16ms  124.31ms   1.77s    95.02%\n    Req/Sec     4.84k     2.82k   34.01k    75.42%\n  574902 requests in 10.10s, 17.55MB read\n  Socket errors: connect 0, read 574902, write 0, timeout 32\nRequests/sec:  56920.29\nTransfer/sec:      1.74MB\n</code></pre>"},{"location":"blog/the-upper-limit-of-wasm-performance/appendix/#c-gcc-o3","title":"C (gcc -O3)","text":"<pre><code>Running 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    18.98ms  115.87ms   1.76s    96.51%\n    Req/Sec     5.00k     2.96k   29.10k    74.96%\n  594236 requests in 10.10s, 18.13MB read\n  Socket errors: connect 0, read 594236, write 0, timeout 28\nRequests/sec:  58834.45\nTransfer/sec:      1.80MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    20.99ms  119.09ms   1.77s    96.18%\n    Req/Sec     5.00k     2.87k   24.91k    74.08%\n  594872 requests in 10.10s, 18.15MB read\n  Socket errors: connect 0, read 594872, write 0, timeout 43\nRequests/sec:  58899.79\nTransfer/sec:      1.80MB\n\nRunning 10s test @ http://127.0.0.1:8080\n  12 threads and 400 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    21.32ms  125.58ms   1.77s    96.48%\n    Req/Sec     5.15k     3.13k   18.28k    71.01%\n  597622 requests in 10.10s, 18.24MB read\n  Socket errors: connect 0, read 597622, write 0, timeout 47\nRequests/sec:  59175.08\nTransfer/sec:      1.81MB\n</code></pre>"},{"location":"demo/","title":"Index","text":""},{"location":"demo/#music","title":"Music","text":"<p>link | Query Element | Action | | :- | :- | | <code>transparent</code> | makes the background trasparent instead of black |</p>"},{"location":"demo/#music-fluid","title":"Music-Fluid","text":"<p>link Uses the frequencies of an audio track to generate currents to simulate particles traveling through. | Query Element | Action | | :- | :- | | <code>transparent</code> | makes the background trasparent instead of black |</p>"},{"location":"demo/#music-sand","title":"Music-Sand","text":"<p>link</p> <p>Currently does not work Uses the frequenceis of an audio track to generate terrain to simulate sand sliding over. Utilises WASM.</p>"},{"location":"demo/music-sand/memory-layout/","title":"Memory layout","text":"Offset Name Type Details 0 count i32 Number of boids 4 width i32 Screen width 8 height i32 Screen height 12 factorAvoid f32 Avoiding factor 16 avoid i32 The avoid range 20 movSpeed f32 Movement speed - - - - n*8 + 24 + 0 posX i32 n*8 + 24 + 4 posY i32"},{"location":"project/","title":"All Projects","text":""},{"location":"project/#programming-languages","title":"Programming Languages","text":"<ul> <li> <p> Uniview  Current </p> <p>View once immutability enabling the safeties of immutable code, while enjoying near procedural performance</p> <p> Source</p> </li> <li> <p> Qupa  2020 </p> <p>Queued Parallelism Language The predecessor to Uniview, a C++ like language designed to encourage implementation design which leads to easily parrallelisable programs.</p> <p> Source</p> </li> <li> <p> Fiber  2018 </p> <p>The original toy language project which compiled to a bytecode, which then ran on a bispoke interpreter to understand language development - which inspired the future language projects I have ventured onto.</p> <p> Source</p> </li> </ul>"},{"location":"project/#visualisations","title":"Visualisations","text":"<ul> <li> <p> Slime Mould  2022 </p> <p>Used GPU shaders to compute a mould simulation. Each spore lays down attractors, and every spore is attracted to those. As the spores move around it creates a cool visualisation.</p> <p> Demo</p> </li> <li> <p> DeepL Chess Notation  2020 </p> <p>This was a group project for a university assignment where we created a web app which you could take a photo of a chess board and it will give you the chess notation of the board. This runs completely on the mobile device with highly usable processing times for keeping board history for a local tournament.</p> <p> Source</p> </li> <li> <p> Dots  2019 </p> <p>Modified boids animation for the front page where the parameters dictating the boids are constantly changing, including into negative values to lead to interesting patterns.</p> <p> Demo</p> </li> <li> <p> Musical Boids  2018 </p> <p>A modification of the dots animation where the parameters (turnspeed, maxspeed, separation, attraction) all change according to the music being played.</p> <p> Demo</p> </li> <li> <p> Musical Vector Field  2018 </p> <p>A large amount of particles are simulated across a vector flow field, where the fields are determined by the frequencies of the music playing.</p> <p> Demo</p> </li> <li> <p> Wiki Web  2016 </p> <p>Give the software a start and an end page, and it will use the Wikipedia APIs to crawl via a bredth first search to find a network of possible routes from A \u2192 B and visualise all of them.</p> <p> Source</p> </li> </ul>"},{"location":"project/#applications","title":"Applications","text":"<ul> <li> <p> Predictable Bot  2023 </p> <p>A discord bot + website to allow people to post predictions allowing people to place wagers on what they think will happen.</p> <p>The whole system was designed to be robust, ensuring no faux money is lost even in the even of an error. And also ensuring that there is zero down time even during upgrades.</p> <p>Ensuring as the old version is shutdown it completes currently active operations while not interrupting the start up of the new version</p> <p> Website</p> </li> <li> <p> Wiki Thesaurus  2023 </p> <p>Uses the simple-wiki dataset to generate a graph structure for wikipedia based on links between articles.</p> <p>From that it then uses Jaccard similarity to take any given input word or entity and attempt to find similar entities. Hopefully producing thesaurus like results (spoiler it's not thesaurus like)</p> <p> Source</p> </li> <li> <p> No Press  2020 </p> <p>A MVP for a drop in replacement for a dying Wordpress sight, which has built in conversion to ingest the pages from a Wordpress site converting it into a statically built blog.</p> <p> Source</p> </li> <li> <p> J Plays  2016 </p> <p>A light weight version of Dj Radio designed only as a local media player built in electron.</p> <p> Source</p> </li> <li> <p> Dj Radio  2016 </p> <p>A NodeJS implementation of a alternative to IceRadio. </p> <p>It will read an entire folder and index the music in it, reading all metadata from the MP3s. The admin can then queue music or allow the server to play random music.</p> <p>The server will re-encode the audio so all devices will see it as a single stream of audio, with server side buffering to fill client side caches on connection for instant stream start, while remaining in sync with other users.</p> <p> Source</p> </li> </ul>"},{"location":"project/#hardware","title":"Hardware","text":"<ul> <li> <p> GKM27  2022 </p> <p>A macropad designed ergonomically for FPS shooters rather than typing. Also includes direct key-to-led mapping allowing reactive lighting visualisations, tuned to maintain a high polling rate.</p> <p> Source</p> </li> </ul>"},{"location":"project/#libraries","title":"Libraries","text":"<p>When I started learning NodeJS, I decided I wouldn't use anything other than the built in libraries to force myself to learn the full features of the language, and to understand how the popular libraries worked and how they were made.</p> <ul> <li> <p> BNF Parser  2021 :typescript-javascript:</p> <p>Takes in a BNF representation of a syntax and compiles it all the way down to a wasm application for parsing that specific syntax with compiler optimisations applied to it.</p> <p>The compiler also generates type-definitions corresponding to the possible syntax tree layout. To create a much more ergonomic developer experience using the outputted syntax, since the structure will be somewhat known.</p> <p> Read More</p> </li> <li> <p> Struct-DB  2018 </p> <p>A simple object oriented database implementation</p> <p> Source</p> </li> <li> <p> FlatFile-DB  2018 </p> <p>A very simple flat table database implementation.</p> <p> Source</p> </li> <li> <p> Theader  2017 </p> <p>Give the API a function, then call the function through the API and it will send the data to a new worker to execute the function on.</p> <p> Source</p> </li> <li> <p> cosf  2017 </p> <p>Compact object storage format. Basically a custom BJSON implementation.</p> <p> Source</p> </li> <li> <p> Passer  2016 </p> <p>A bispoke express like library with the ability for: wildcards in addresses; cookie decode/encode; query string decode/encode; multi-part form parsing (including files); session management; authorisation checks for certain url paths.</p> <p> Source</p> </li> <li> <p> Custom Radix  2016 </p> <p>Define a character set for your radix, then it can convert to and from that stringified form.</p> <p> Source</p> </li> <li> <p> Adatre  2016 </p> <p>Asynchronous class-based referencing databasse</p> <p> Source</p> </li> <li> <p> Mass Random  2016 </p> <p>Bounded random values, and random string generation</p> <p> Source</p> </li> <li> <p> Object Manipulation  2016 </p> <p>Difference of objects, merge two objects, deep copy of objects.</p> <p> Source</p> </li> </ul>"},{"location":"project/#other","title":"Other","text":"<ul> <li> <p> Woona IRC Bot  2016 </p> <p>A basic IRC bot which oberves how users react to it, and memorises those responses and attempts to use them later to respond to the user.</p> <p> Source</p> </li> </ul>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#apex-legends","title":"Apex Legends","text":"<ul> <li>A Case Study on Apex SBMM</li> </ul>"},{"location":"tags/#asynchronous","title":"Asynchronous","text":"<ul> <li>Async functions are needlessly killing your Javascript performance</li> </ul>"},{"location":"tags/#case-study","title":"Case-Study","text":"<ul> <li>Case Study: Guess Who</li> </ul>"},{"location":"tags/#data-structure","title":"Data-Structure","text":"<ul> <li>Relational Table DBMS</li> <li>Object Orientated Database (Static Schema)</li> <li>Buddy Memory Allocation</li> </ul>"},{"location":"tags/#database","title":"Database","text":"<ul> <li>Implementing Database level garbage collection for Blobs</li> </ul>"},{"location":"tags/#garbage-collection","title":"Garbage Collection","text":"<ul> <li>Implementing Database level garbage collection for Blobs</li> </ul>"},{"location":"tags/#ipt","title":"IPT","text":"<ul> <li>Weighted Random</li> <li>Relational Table DBMS</li> <li>Key VS Value Hashing</li> <li>Object Orientated Database (Static Schema)</li> <li>Case Study: Guess Who</li> <li>Buddy Memory Allocation</li> </ul>"},{"location":"tags/#javascript","title":"Javascript","text":"<ul> <li>Async functions are needlessly killing your Javascript performance</li> <li>Wasm is not going to save Javascript</li> </ul>"},{"location":"tags/#match-making","title":"Match Making","text":"<ul> <li>A Case Study on Apex SBMM</li> </ul>"},{"location":"tags/#optimisation","title":"Optimisation","text":"<ul> <li>Async functions are needlessly killing your Javascript performance</li> </ul>"},{"location":"tags/#performance","title":"Performance","text":"<ul> <li>Async functions are needlessly killing your Javascript performance</li> <li>Wasm is not going to save Javascript</li> </ul>"},{"location":"tags/#promise","title":"Promise","text":"<ul> <li>Async functions are needlessly killing your Javascript performance</li> </ul>"},{"location":"tags/#s3-storage","title":"S3 Storage","text":"<ul> <li>Implementing Database level garbage collection for Blobs</li> </ul>"},{"location":"tags/#sbmm","title":"SBMM","text":"<ul> <li>A Case Study on Apex SBMM</li> </ul>"},{"location":"tags/#sql","title":"SQL","text":"<ul> <li>Implementing Database level garbage collection for Blobs</li> </ul>"},{"location":"tags/#web-assembly","title":"Web Assembly","text":"<ul> <li>Wasm is not going to save Javascript</li> </ul>"},{"location":"tags/#work-flow","title":"Work-flow","text":"<ul> <li>Tabs VS Spaces</li> </ul>"},{"location":"tags/#http","title":"http","text":"<ul> <li>The Upper Limits of WebAssembly Net Performance</li> </ul>"},{"location":"tags/#performance_1","title":"performance","text":"<ul> <li>The Upper Limits of WebAssembly Net Performance</li> </ul>"},{"location":"tags/#wasm","title":"wasm","text":"<ul> <li>Wasm is not going to save Javascript</li> </ul>"},{"location":"tags/#web-assembly_1","title":"web assembly","text":"<ul> <li>The Upper Limits of WebAssembly Net Performance</li> </ul>"}]}